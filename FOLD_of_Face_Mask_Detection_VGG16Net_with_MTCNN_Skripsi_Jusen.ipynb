{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44JRqlK3IyFv"
      },
      "source": [
        "**Face Mask Detection**\n",
        "\n",
        "Program Machine Learning untuk mendeteksi penggunaan masker. Program dibuat menggunakan metode CNN dengan arsitektur VGG16Net dan MTCNN untuk face detection.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvjSZ30YhWh"
      },
      "source": [
        "## Mengimpor Libraries yang dibutuhkan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Za-KOKf-Ot",
        "outputId": "24adee42-c900-4da4-e315-be0b6ebae6a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "Mon Aug 14 11:14:37 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1050      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   41C    P0              N/A / ERR! |    140MiB /  3072MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     12072      C   ...\\georg\\anaconda3\\envs\\tf\\python.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU5Fqk2vLELg"
      },
      "source": [
        "## Preprocessing Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D48VICs21pna",
        "outputId": "cea2273f-1fce-4e75-d91a-b7d54b84edf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menginput gambar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input gambar berhasil\n"
          ]
        }
      ],
      "source": [
        "# Inisialisasi nilai Initial Learning Rate, berapa banyak Epoch pelatihan, dan Batch Size\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 100\n",
        "BS = 32\n",
        " \n",
        "# Mengambil gambar dari dataset directory, kemudian inisialisasi data dan class gambar\n",
        "print(\"Menginput gambar...\")\n",
        "\n",
        "imagePaths = list(paths.list_images('dataset'))\n",
        "data = []\n",
        "labels = []\n",
        " \n",
        "# Melakukan perulangan pada image paths\n",
        "for imagePath in imagePaths:\n",
        " \n",
        "    # Mengekstrak class label dari filename\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    # Memuat input gambar (224x224) dan melakukan proses\n",
        "    image = load_img(imagePath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        " \n",
        "    # Mengupdate data dan labels lists, berurutan\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        " \n",
        "# Mengkonversi data dan label ke dalam NumPy Arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        " \n",
        "# Melakukan one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "print(\"Input gambar berhasil\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aF2vfrgni3tq"
      },
      "source": [
        "### Membuat objek ImageDataGenerator dan Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRBrygye5yvc",
        "outputId": "cbf33c6e-2b61-4896-a861-b723597e9581"
      },
      "outputs": [],
      "source": [
        "# Mempartisi data ke dalam pelatihan dan pengujian ( 75% : 25% )\n",
        "'''\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "    test_size=0.25, stratify=labels, random_state=42)\n",
        " '''\n",
        "\n",
        "\n",
        "# Membentuk training image generator untuk data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPD753f5205"
      },
      "source": [
        "## Membuat Model Jaringan CNN yang sudah dipelajari sebelumnya (pre-trained convnets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaDceTqP6HLr",
        "outputId": "549de847-15bc-46e9-f9d8-e024c5fd2a89"
      },
      "outputs": [],
      "source": [
        "# Arsitektur jaringan VGG16Net\n",
        "baseModel = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NfSLr2q2LWRZ"
      },
      "source": [
        "### Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0U2qYnO6KYW",
        "outputId": "00a32ad0-22a0-42a6-8495-b1527f745d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "baseModel.trainable = False\n",
        "baseModel.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ENpg9P6Q6SpL"
      },
      "source": [
        "## Tahap Pembuatan Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 1, 1, 512)        0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,780,610\n",
            "Trainable params: 65,922\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Membentuk bagian head dari model yang akan ditempatkan pada base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        " \n",
        "# Menempatkan head model pada base model\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXEyXB5NTU13",
        "outputId": "c5f827c0-3a25-4e98-8b38-2d9a88076619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mengkompilasi model...\n",
            "fold 1\n",
            "Training head model...\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - 83s 717ms/step - loss: 0.6587 - accuracy: 0.6414 - val_loss: 0.5593 - val_accuracy: 0.9179\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - 45s 468ms/step - loss: 0.5489 - accuracy: 0.8078 - val_loss: 0.4602 - val_accuracy: 0.9361\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - 45s 467ms/step - loss: 0.4630 - accuracy: 0.8813 - val_loss: 0.3753 - val_accuracy: 0.9465\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - 45s 469ms/step - loss: 0.3962 - accuracy: 0.9061 - val_loss: 0.3112 - val_accuracy: 0.9505\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - 45s 470ms/step - loss: 0.3331 - accuracy: 0.9249 - val_loss: 0.2669 - val_accuracy: 0.9478\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - 45s 470ms/step - loss: 0.3022 - accuracy: 0.9219 - val_loss: 0.2288 - val_accuracy: 0.9544\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - 43s 455ms/step - loss: 0.2657 - accuracy: 0.9367 - val_loss: 0.2016 - val_accuracy: 0.9544\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - 44s 460ms/step - loss: 0.2374 - accuracy: 0.9380 - val_loss: 0.1841 - val_accuracy: 0.9544\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - 44s 466ms/step - loss: 0.2235 - accuracy: 0.9380 - val_loss: 0.1631 - val_accuracy: 0.9570\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - 45s 474ms/step - loss: 0.2071 - accuracy: 0.9430 - val_loss: 0.1507 - val_accuracy: 0.9570\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - 44s 465ms/step - loss: 0.1870 - accuracy: 0.9497 - val_loss: 0.1395 - val_accuracy: 0.9596\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - 43s 451ms/step - loss: 0.1805 - accuracy: 0.9535 - val_loss: 0.1281 - val_accuracy: 0.9609\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - 44s 457ms/step - loss: 0.1693 - accuracy: 0.9509 - val_loss: 0.1189 - val_accuracy: 0.9648\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - 45s 469ms/step - loss: 0.1654 - accuracy: 0.9532 - val_loss: 0.1135 - val_accuracy: 0.9661\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - 43s 453ms/step - loss: 0.1507 - accuracy: 0.9529 - val_loss: 0.1110 - val_accuracy: 0.9661\n",
            "Epoch 16/100\n",
            "95/95 [==============================] - 43s 453ms/step - loss: 0.1430 - accuracy: 0.9631 - val_loss: 0.1032 - val_accuracy: 0.9674\n",
            "Epoch 17/100\n",
            "95/95 [==============================] - 44s 459ms/step - loss: 0.1369 - accuracy: 0.9598 - val_loss: 0.0986 - val_accuracy: 0.9687\n",
            "Epoch 18/100\n",
            "95/95 [==============================] - 46s 480ms/step - loss: 0.1355 - accuracy: 0.9595 - val_loss: 0.0931 - val_accuracy: 0.9713\n",
            "Epoch 19/100\n",
            "95/95 [==============================] - 51s 538ms/step - loss: 0.1341 - accuracy: 0.9624 - val_loss: 0.0900 - val_accuracy: 0.9713\n",
            "Epoch 20/100\n",
            "95/95 [==============================] - 50s 522ms/step - loss: 0.1255 - accuracy: 0.9604 - val_loss: 0.0859 - val_accuracy: 0.9726\n",
            "Epoch 21/100\n",
            "95/95 [==============================] - 49s 516ms/step - loss: 0.1184 - accuracy: 0.9651 - val_loss: 0.0842 - val_accuracy: 0.9739\n",
            "Epoch 22/100\n",
            "95/95 [==============================] - 46s 477ms/step - loss: 0.1179 - accuracy: 0.9595 - val_loss: 0.0838 - val_accuracy: 0.9713\n",
            "Epoch 23/100\n",
            "95/95 [==============================] - 44s 462ms/step - loss: 0.1154 - accuracy: 0.9618 - val_loss: 0.0783 - val_accuracy: 0.9765\n",
            "Epoch 24/100\n",
            "95/95 [==============================] - 45s 476ms/step - loss: 0.1095 - accuracy: 0.9670 - val_loss: 0.0757 - val_accuracy: 0.9765\n",
            "Epoch 25/100\n",
            "95/95 [==============================] - 44s 458ms/step - loss: 0.1047 - accuracy: 0.9687 - val_loss: 0.0731 - val_accuracy: 0.9791\n",
            "Epoch 26/100\n",
            "95/95 [==============================] - 47s 491ms/step - loss: 0.1056 - accuracy: 0.9654 - val_loss: 0.0711 - val_accuracy: 0.9791\n",
            "Epoch 27/100\n",
            "95/95 [==============================] - 45s 468ms/step - loss: 0.0995 - accuracy: 0.9680 - val_loss: 0.0692 - val_accuracy: 0.9778\n",
            "Epoch 28/100\n",
            "95/95 [==============================] - 43s 450ms/step - loss: 0.1008 - accuracy: 0.9680 - val_loss: 0.0674 - val_accuracy: 0.9791\n",
            "Epoch 29/100\n",
            "95/95 [==============================] - 43s 455ms/step - loss: 0.0970 - accuracy: 0.9697 - val_loss: 0.0656 - val_accuracy: 0.9791\n",
            "Epoch 30/100\n",
            "95/95 [==============================] - 45s 467ms/step - loss: 0.0939 - accuracy: 0.9730 - val_loss: 0.0638 - val_accuracy: 0.9791\n",
            "Epoch 31/100\n",
            "95/95 [==============================] - 46s 485ms/step - loss: 0.0896 - accuracy: 0.9740 - val_loss: 0.0625 - val_accuracy: 0.9791\n",
            "Epoch 32/100\n",
            "95/95 [==============================] - 44s 459ms/step - loss: 0.0883 - accuracy: 0.9723 - val_loss: 0.0611 - val_accuracy: 0.9831\n",
            "Epoch 33/100\n",
            "95/95 [==============================] - 44s 462ms/step - loss: 0.0863 - accuracy: 0.9746 - val_loss: 0.0624 - val_accuracy: 0.9804\n",
            "Epoch 34/100\n",
            "95/95 [==============================] - 44s 460ms/step - loss: 0.0798 - accuracy: 0.9733 - val_loss: 0.0585 - val_accuracy: 0.9844\n",
            "Epoch 35/100\n",
            "95/95 [==============================] - 46s 480ms/step - loss: 0.0829 - accuracy: 0.9717 - val_loss: 0.0576 - val_accuracy: 0.9831\n",
            "Epoch 36/100\n",
            "95/95 [==============================] - 44s 464ms/step - loss: 0.0835 - accuracy: 0.9736 - val_loss: 0.0565 - val_accuracy: 0.9831\n",
            "Epoch 37/100\n",
            "95/95 [==============================] - 45s 469ms/step - loss: 0.0816 - accuracy: 0.9736 - val_loss: 0.0560 - val_accuracy: 0.9844\n",
            "Epoch 38/100\n",
            "95/95 [==============================] - 44s 463ms/step - loss: 0.0778 - accuracy: 0.9763 - val_loss: 0.0547 - val_accuracy: 0.9857\n",
            "Epoch 39/100\n",
            "95/95 [==============================] - 50s 521ms/step - loss: 0.0802 - accuracy: 0.9746 - val_loss: 0.0538 - val_accuracy: 0.9844\n",
            "Epoch 40/100\n",
            "95/95 [==============================] - 46s 482ms/step - loss: 0.0761 - accuracy: 0.9763 - val_loss: 0.0531 - val_accuracy: 0.9844\n",
            "Epoch 41/100\n",
            "95/95 [==============================] - 49s 513ms/step - loss: 0.0773 - accuracy: 0.9757 - val_loss: 0.0521 - val_accuracy: 0.9844\n",
            "Epoch 42/100\n",
            "95/95 [==============================] - 46s 487ms/step - loss: 0.0720 - accuracy: 0.9782 - val_loss: 0.0513 - val_accuracy: 0.9857\n",
            "Epoch 43/100\n",
            "95/95 [==============================] - 46s 478ms/step - loss: 0.0745 - accuracy: 0.9773 - val_loss: 0.0511 - val_accuracy: 0.9857\n",
            "Epoch 44/100\n",
            "95/95 [==============================] - 46s 484ms/step - loss: 0.0680 - accuracy: 0.9773 - val_loss: 0.0495 - val_accuracy: 0.9844\n",
            "Epoch 45/100\n",
            "95/95 [==============================] - 48s 500ms/step - loss: 0.0765 - accuracy: 0.9786 - val_loss: 0.0490 - val_accuracy: 0.9857\n",
            "Epoch 46/100\n",
            "95/95 [==============================] - 47s 492ms/step - loss: 0.0693 - accuracy: 0.9786 - val_loss: 0.0490 - val_accuracy: 0.9857\n",
            "Epoch 47/100\n",
            "95/95 [==============================] - 46s 486ms/step - loss: 0.0689 - accuracy: 0.9756 - val_loss: 0.0482 - val_accuracy: 0.9857\n",
            "Epoch 48/100\n",
            "95/95 [==============================] - 47s 497ms/step - loss: 0.0666 - accuracy: 0.9779 - val_loss: 0.0477 - val_accuracy: 0.9857\n",
            "Epoch 49/100\n",
            "95/95 [==============================] - 51s 538ms/step - loss: 0.0697 - accuracy: 0.9776 - val_loss: 0.0469 - val_accuracy: 0.9870\n",
            "Epoch 50/100\n",
            "95/95 [==============================] - 51s 535ms/step - loss: 0.0635 - accuracy: 0.9809 - val_loss: 0.0464 - val_accuracy: 0.9857\n",
            "Epoch 51/100\n",
            "95/95 [==============================] - 47s 488ms/step - loss: 0.0616 - accuracy: 0.9812 - val_loss: 0.0457 - val_accuracy: 0.9857\n",
            "Epoch 52/100\n",
            "95/95 [==============================] - 46s 485ms/step - loss: 0.0643 - accuracy: 0.9776 - val_loss: 0.0468 - val_accuracy: 0.9870\n",
            "Epoch 53/100\n",
            "95/95 [==============================] - 48s 500ms/step - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.0461 - val_accuracy: 0.9857\n",
            "Epoch 54/100\n",
            "95/95 [==============================] - 45s 474ms/step - loss: 0.0626 - accuracy: 0.9786 - val_loss: 0.0448 - val_accuracy: 0.9857\n",
            "Epoch 55/100\n",
            "95/95 [==============================] - 46s 483ms/step - loss: 0.0623 - accuracy: 0.9799 - val_loss: 0.0454 - val_accuracy: 0.9870\n",
            "Epoch 56/100\n",
            "95/95 [==============================] - 47s 496ms/step - loss: 0.0587 - accuracy: 0.9819 - val_loss: 0.0434 - val_accuracy: 0.9870\n",
            "Epoch 57/100\n",
            "95/95 [==============================] - 45s 471ms/step - loss: 0.0625 - accuracy: 0.9786 - val_loss: 0.0438 - val_accuracy: 0.9857\n",
            "Epoch 58/100\n",
            "95/95 [==============================] - 46s 480ms/step - loss: 0.0679 - accuracy: 0.9786 - val_loss: 0.0431 - val_accuracy: 0.9870\n",
            "Epoch 59/100\n",
            "95/95 [==============================] - 43s 449ms/step - loss: 0.0591 - accuracy: 0.9815 - val_loss: 0.0422 - val_accuracy: 0.9870\n",
            "Epoch 60/100\n",
            "95/95 [==============================] - 43s 449ms/step - loss: 0.0590 - accuracy: 0.9815 - val_loss: 0.0424 - val_accuracy: 0.9857\n",
            "Epoch 61/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0592 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9857\n",
            "Epoch 62/100\n",
            "95/95 [==============================] - 43s 448ms/step - loss: 0.0570 - accuracy: 0.9812 - val_loss: 0.0418 - val_accuracy: 0.9844\n",
            "Epoch 63/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0563 - accuracy: 0.9812 - val_loss: 0.0413 - val_accuracy: 0.9896\n",
            "Epoch 64/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0556 - accuracy: 0.9819 - val_loss: 0.0404 - val_accuracy: 0.9870\n",
            "Epoch 65/100\n",
            "95/95 [==============================] - 42s 445ms/step - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0413 - val_accuracy: 0.9857\n",
            "Epoch 66/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0570 - accuracy: 0.9782 - val_loss: 0.0397 - val_accuracy: 0.9883\n",
            "Epoch 67/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0576 - accuracy: 0.9806 - val_loss: 0.0400 - val_accuracy: 0.9844\n",
            "Epoch 68/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0510 - accuracy: 0.9806 - val_loss: 0.0399 - val_accuracy: 0.9857\n",
            "Epoch 69/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0554 - accuracy: 0.9819 - val_loss: 0.0391 - val_accuracy: 0.9870\n",
            "Epoch 70/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0537 - accuracy: 0.9825 - val_loss: 0.0389 - val_accuracy: 0.9857\n",
            "Epoch 71/100\n",
            "95/95 [==============================] - 43s 447ms/step - loss: 0.0552 - accuracy: 0.9799 - val_loss: 0.0392 - val_accuracy: 0.9896\n",
            "Epoch 72/100\n",
            "95/95 [==============================] - 43s 450ms/step - loss: 0.0465 - accuracy: 0.9868 - val_loss: 0.0385 - val_accuracy: 0.9857\n",
            "Epoch 73/100\n",
            "95/95 [==============================] - 42s 445ms/step - loss: 0.0487 - accuracy: 0.9829 - val_loss: 0.0381 - val_accuracy: 0.9883\n",
            "Epoch 74/100\n",
            "95/95 [==============================] - 42s 442ms/step - loss: 0.0517 - accuracy: 0.9825 - val_loss: 0.0377 - val_accuracy: 0.9896\n",
            "Epoch 75/100\n",
            "95/95 [==============================] - 43s 448ms/step - loss: 0.0514 - accuracy: 0.9819 - val_loss: 0.0374 - val_accuracy: 0.9896\n",
            "Epoch 76/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0479 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9883\n",
            "Epoch 77/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0500 - accuracy: 0.9845 - val_loss: 0.0375 - val_accuracy: 0.9870\n",
            "Epoch 78/100\n",
            "95/95 [==============================] - 43s 449ms/step - loss: 0.0450 - accuracy: 0.9865 - val_loss: 0.0380 - val_accuracy: 0.9870\n",
            "Epoch 79/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0463 - accuracy: 0.9858 - val_loss: 0.0377 - val_accuracy: 0.9857\n",
            "Epoch 80/100\n",
            "95/95 [==============================] - 43s 455ms/step - loss: 0.0474 - accuracy: 0.9842 - val_loss: 0.0364 - val_accuracy: 0.9870\n",
            "Epoch 81/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0424 - accuracy: 0.9858 - val_loss: 0.0360 - val_accuracy: 0.9883\n",
            "Epoch 82/100\n",
            "95/95 [==============================] - 43s 447ms/step - loss: 0.0462 - accuracy: 0.9822 - val_loss: 0.0360 - val_accuracy: 0.9883\n",
            "Epoch 83/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0480 - accuracy: 0.9835 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
            "Epoch 84/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0514 - accuracy: 0.9832 - val_loss: 0.0362 - val_accuracy: 0.9857\n",
            "Epoch 85/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0441 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9883\n",
            "Epoch 86/100\n",
            "95/95 [==============================] - 42s 445ms/step - loss: 0.0446 - accuracy: 0.9825 - val_loss: 0.0354 - val_accuracy: 0.9883\n",
            "Epoch 87/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.0352 - val_accuracy: 0.9883\n",
            "Epoch 88/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0402 - accuracy: 0.9848 - val_loss: 0.0354 - val_accuracy: 0.9883\n",
            "Epoch 89/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0413 - accuracy: 0.9868 - val_loss: 0.0345 - val_accuracy: 0.9883\n",
            "Epoch 90/100\n",
            "95/95 [==============================] - 43s 447ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.0353 - val_accuracy: 0.9857\n",
            "Epoch 91/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0488 - accuracy: 0.9852 - val_loss: 0.0351 - val_accuracy: 0.9857\n",
            "Epoch 92/100\n",
            "95/95 [==============================] - 43s 446ms/step - loss: 0.0425 - accuracy: 0.9858 - val_loss: 0.0353 - val_accuracy: 0.9857\n",
            "Epoch 93/100\n",
            "95/95 [==============================] - 42s 442ms/step - loss: 0.0430 - accuracy: 0.9835 - val_loss: 0.0339 - val_accuracy: 0.9896\n",
            "Epoch 94/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0413 - accuracy: 0.9875 - val_loss: 0.0342 - val_accuracy: 0.9883\n",
            "Epoch 95/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0438 - accuracy: 0.9835 - val_loss: 0.0356 - val_accuracy: 0.9857\n",
            "Epoch 96/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0397 - accuracy: 0.9885 - val_loss: 0.0341 - val_accuracy: 0.9870\n",
            "Epoch 97/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0428 - accuracy: 0.9835 - val_loss: 0.0339 - val_accuracy: 0.9883\n",
            "Epoch 98/100\n",
            "95/95 [==============================] - 42s 444ms/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 0.0330 - val_accuracy: 0.9896\n",
            "Epoch 99/100\n",
            "95/95 [==============================] - 42s 445ms/step - loss: 0.0444 - accuracy: 0.9858 - val_loss: 0.0346 - val_accuracy: 0.9870\n",
            "Epoch 100/100\n",
            "95/95 [==============================] - 42s 443ms/step - loss: 0.0426 - accuracy: 0.9842 - val_loss: 0.0337 - val_accuracy: 0.9896\n",
            "Mengkompilasi model...\n",
            "fold 2\n",
            "Training head model...\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9858"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "Graph execution error:\n\nDetected at node 'model/block1_conv2/Relu' defined at (most recent call last):\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_12072\\860264005.py\", line 32, in <module>\n      H = model.fit(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n      val_logs = self.evaluate(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n      y_pred = self(x, training=False)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model/block1_conv2/Relu'\nOOM when allocating tensor with shape[34,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/block1_conv2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_test_function_52242]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m#model.summary()\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining head model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m H \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     33\u001b[0m   aug\u001b[39m.\u001b[39;49mflow(train_data, train_labels, batch_size\u001b[39m=\u001b[39;49mBS),\n\u001b[0;32m     34\u001b[0m   steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_data) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BS,\n\u001b[0;32m     35\u001b[0m   validation_data\u001b[39m=\u001b[39;49m(test_data, test_labels),\n\u001b[0;32m     36\u001b[0m   validation_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(test_data) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BS,\n\u001b[0;32m     37\u001b[0m   epochs\u001b[39m=\u001b[39;49mEPOCHS)\n\u001b[0;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mH = model.fit(aug.flow(train_data,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m        steps_per_epoch=len(train_data) // BS,\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m        validation_data=test_data,\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m        epochs=10))\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     45\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_data, test_labels, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/block1_conv2/Relu' defined at (most recent call last):\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_12072\\860264005.py\", line 32, in <module>\n      H = model.fit(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n      val_logs = self.evaluate(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n      y_pred = self(x, training=False)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"c:\\Users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model/block1_conv2/Relu'\nOOM when allocating tensor with shape[34,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/block1_conv2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_test_function_52242]"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle =True)\n",
        "\n",
        "(train_data, test_data, train_labels, test_labels) = train_test_split(data, labels,\n",
        "    test_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fold_accuracies = []\n",
        " \n",
        "# Perulangan pada seluruh base model\n",
        "fold = 1\n",
        "for train_index, test_index in kf.split(data):\n",
        "  print(\"Mengkompilasi model...\")\n",
        "  print(\"fold\", fold)\n",
        " \n",
        " \n",
        " \n",
        "  train_data, train_labels = data[train_index], labels[train_index]\n",
        "  test_data, test_labels = data[test_index], labels[test_index]\n",
        "  \n",
        "  \n",
        "  opt = tf.keras.optimizers.legacy.Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "  \n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "    metrics=[\"accuracy\"])\n",
        "  #model.summary()\n",
        "\n",
        "  print(\"Training head model...\")\n",
        "\n",
        "  H = model.fit(\n",
        "    aug.flow(train_data, train_labels, batch_size=BS),\n",
        "    steps_per_epoch=len(train_data) // BS,\n",
        "    validation_data=(test_data, test_labels),\n",
        "    validation_steps=len(test_data) // BS,\n",
        "    epochs=EPOCHS)\n",
        "  \n",
        "  '''\n",
        "  H = model.fit(aug.flow(train_data,\n",
        "          steps_per_epoch=len(train_data) // BS,\n",
        "          validation_data=test_data,\n",
        "          epochs=10))\n",
        "  '''\n",
        "  loss, accuracy = model.evaluate(test_data, test_labels, verbose = 0)\n",
        "  \n",
        "  fold_accuracies.append(accuracy)\n",
        "\n",
        "  fold+=1\n",
        "  \n",
        "'''\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 30\n",
        "BS = 32\n",
        "'''  \n",
        "# H = model.fit(aug.flow(train_data, train_labels, epochs = 10, batch_size = 32, verbose=0))\n",
        "\n",
        "\n",
        "#for layer in baseModel.layers:\n",
        "    #layer.trainable = False\n",
        " \n",
        "# Persiapan kompilasi model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lH42Hp3UWrPs"
      },
      "source": [
        "### Melakukan Pelatihan Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZPSj-se6WdI",
        "outputId": "b131fccc-690f-4d9c-d5bb-079363daca12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95.95300555229187\n",
            "0.9595300555229187\n"
          ]
        }
      ],
      "source": [
        "# Pelatihan model\n",
        "mean_accuracy = np.mean(accuracy)\n",
        "print(mean_accuracy*100)\n",
        "print(accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MxqTwaGi7Wzs"
      },
      "source": [
        "## Menampilkan Grafik Model Hasil Pelatihan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_accs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m----> 5\u001b[0m     plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epoch_accs\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), epoch_accs[fold], label\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epoch_accs = np.array(fold_accuracies)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "for fold in range(epoch_accs.shape[0]):\n",
        "    plt.plot(range(1, epoch_accs.shape[1]+1), epoch_accs[fold], label=f\"Fold {fold+1}\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy per Epoch - K-Fold Cross Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "OURpwhV87ac9",
        "outputId": "2321944e-4977-419e-a3f6-921b3801a1a0"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (30,) and (2,)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m fig\u001b[39m.\u001b[39mset_figwidth(\u001b[39m15\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m plt\u001b[39m.\u001b[39;49mplot(np\u001b[39m.\u001b[39;49marange(\u001b[39m0\u001b[39;49m, N), H\u001b[39m.\u001b[39;49mhistory[\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m],label \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mTraining Accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, N), H\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m],label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mValidation Accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
            "File \u001b[1;32mc:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[0;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[0;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
            "File \u001b[1;32mc:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
            "File \u001b[1;32mc:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (30,) and (2,)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "N = 2\n",
        "#N = EPOCHS\n",
        "fig = plt.figure(figsize=(7, 4))\n",
        "fig.set_figheight(10)\n",
        "fig.set_figwidth(15)\n",
        " \n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"],label = \"Training Accuracy\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"],label = \"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Akurasi\")\n",
        "plt.title(\"Kurva Tingkat Akurasi\", size=15)\n",
        "plt.grid(zorder = 0)\n",
        " \n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"],label = \"Training Loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"],label = \"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Kurva Tingkat Error\", size=15)\n",
        "plt.grid(zorder = 0)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mqe8nNB7lp8"
      },
      "source": [
        "## Evaluasi Jaringan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLxROO1pg7Em",
        "outputId": "a1c09ec8-8171-4997-ad03-415a0327d317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n",
            "96/96 [==============================] - 753s 8s/step - loss: 0.1480 - accuracy: 0.9641\n",
            "[0.14803509414196014, 0.9641343355178833]\n"
          ]
        }
      ],
      "source": [
        "# Memeriksa matriks model\n",
        "print(model.metrics_names)\n",
        "# Evaluasi data test\n",
        "'''\n",
        "print(model.evaluate(x= testX, y = testY))\n",
        "'''\n",
        "\n",
        "print(model.evaluate(x= train_data, y = train_labels))\n",
        "# evaluasi fold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySJujhciZvlQ",
        "outputId": "2a63ec46-9249-4de9-cd1e-fb45cf24ab7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 759s 8s/step\n",
            "[1 1 1 ... 0 0 0]\n",
            "[0 1 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan matriks yang benar dan matriks hasil prediksi\n",
        "# Label yang benar\n",
        "yTrue = np.argmax(train_labels, axis=1)\n",
        "\n",
        "# Label prediksi\n",
        "YPred = model.predict(train_data, batch_size=BS)\n",
        "yPred = np.argmax(YPred, axis=1)\n",
        "\n",
        "print(yTrue)\n",
        "print(yPred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yl0Y-D41imCP"
      },
      "source": [
        "## Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL_ROzJdh6pm",
        "outputId": "d85cbdc9-735a-47f3-dd2b-ebaa9d705de3"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(yTrue, yPred):\n",
        "    n_classes = len(np.unique(yTrue)) \n",
        "    conf = np.zeros((n_classes, n_classes))\n",
        "    for actual, pred in zip(yTrue, yPred):\n",
        "        conf[int(actual)][int(pred)] += 1\n",
        "    return conf.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxoC6xxjaCrf",
        "outputId": "789d26ad-0fe5-42b4-b099-84c5631bd977"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1485,   59],\n",
              "       [  51, 1472]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conf = get_confusion_matrix(yTrue, yPred)\n",
        "conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Ss9YmprkdirH",
        "outputId": "a46b056d-74e4-4028-c5b4-30566ad16782"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 23.52222222222222, 'Label Prediksi')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "classes = [0, 1]\n",
        "# Plot confusion matrix\n",
        "plt.imshow(conf, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "fmt = 'd'\n",
        "thresh = conf.max() / 2.\n",
        "for i, j in itertools.product(range(conf.shape[0]), range(conf.shape[1])):\n",
        "    plt.text(j, i, format(conf[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if conf[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('Label benar')\n",
        "plt.xlabel('Label Prediksi')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr58aW7Ui7PD"
      },
      "source": [
        "Analisis mAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Psuc1y0i_NJ",
        "outputId": "9f02b5b3-6a7b-465c-832f-9f1cadb8c991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 973 µs (started: 2023-05-05 03:20:09 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Berdasarkan confusion matrix\n",
        "TP = true_pos = 366\n",
        "TN = true_neg = 380\n",
        "FP = false_pos = 17\n",
        "FN = false_neg = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YorxeJVtj0tZ",
        "outputId": "30480640-834a-4342-d107-0637d29de2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi =  0.973\n",
            "Recall =  0.989\n",
            "Presisi =  0.956\n",
            "F1 =  0.972\n",
            "time: 6.78 ms (started: 2023-05-05 03:20:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "# Akurasi\n",
        "metric = \"Akurasi\"\n",
        "results[metric] = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Recall\n",
        "metric = \"Recall\"\n",
        "results[metric] = TP / (TP + FN)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Presisi\n",
        "metric = \"Presisi\"\n",
        "results[metric] = TP / (TP + FP)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Nilai F1\n",
        "metric = \"F1\"\n",
        "results[metric] = 2 / (1 / results[\"Presisi\"] + 1 / results[\"Recall\"])\n",
        "print(f\"{metric} = {results[metric]: .3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDPEy5zamOoi",
        "outputId": "cf046df8-a326-4673-82da-695545bba41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 723s 8s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   with_mask       0.97      0.96      0.96      1544\n",
            "without_mask       0.96      0.97      0.96      1523\n",
            "\n",
            "    accuracy                           0.96      3067\n",
            "   macro avg       0.96      0.96      0.96      3067\n",
            "weighted avg       0.96      0.96      0.96      3067\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Membuat prediksi dari pengujian\n",
        "predIdxs = model.predict(train_data, batch_size=BS)\n",
        " \n",
        "# Untuk setiap gambar dalam set pengujian, kita perlu menemukan indeks label\n",
        "# dengan probabilitas prediksi terbesar\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        " \n",
        "# Menampilkan laporan klasifikasi yang diformat dengan baik\n",
        "print(classification_report(train_labels.argmax(axis=1), predIdxs,\n",
        "    target_names=lb.classes_))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S8U7B1jv8ZQr"
      },
      "source": [
        "## Menyimpan dan Konversi Model ke \".tflite\"\n",
        "\n",
        "Menyimpan model menggunakan tf.saved_model/save dan kemudian mengkonversi model tersimpan ke format yang kompatibel tf lite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTaHZm7mKjLK",
        "outputId": "6ac60462-c83c-47e0-f4d3-8b63d564ea04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1\\assets\n"
          ]
        }
      ],
      "source": [
        "export_dir='saved_model/1'\n",
        "tf.saved_model.save(model, export_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQDDJ2eYKk21",
        "outputId": "71e8079f-6c1f-466d-d921-a16bd04c9f9a"
      },
      "outputs": [],
      "source": [
        "# Mengkonvert model ke format tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZet8SWWKn_S",
        "outputId": "47bd6c94-dd1e-4b32-a605-ec38717c9155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "59131992"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menyimpan model\n",
        "tflite_model_file = pathlib.Path('face-mask-detection/content/model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znn1rBf6Jcfu",
        "outputId": "7056277f-cacb-4493-a785-ff7d902d0742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall:0', 'index': 54, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ],
      "source": [
        "# Memuat model dan mengalokasikan ke tensor\n",
        "interpreter = tf.lite.Interpreter(model_content = tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Mendapatkan input dan ouput tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "print(input_details)\n",
        "print(output_details)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D-H94fpq_E0J"
      },
      "source": [
        "# Pengujian Model dengan MTCNN\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0GaCA5_SRw"
      },
      "source": [
        "Model diujikan pada gambar dan secara real-time dengan menggunakan MTCNN yang digunakan untuk mendeteksi wajah.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVWYYLdvZJjP",
        "outputId": "b4c1fefd-990e-4ba5-e116-03813d253557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "  Using cached mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "Requirement already satisfied: keras>=2.0.0 in c:\\users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages (from mtcnn) (2.10.0)\n",
            "Collecting opencv-python>=4.1.0\n",
            "  Using cached opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl (38.2 MB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\georg\\anaconda3\\envs\\tf\\lib\\site-packages (from opencv-python>=4.1.0->mtcnn) (1.24.3)\n",
            "Installing collected packages: opencv-python, mtcnn\n",
            "Successfully installed mtcnn-0.1.1 opencv-python-4.7.0.72\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nej9QmRT8-DO"
      },
      "source": [
        "## Penggunaan Model pada Gambar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwPKPee0Qi3h",
        "outputId": "15aa788d-3cfd-46c3-af2e-96792ca1289a"
      },
      "outputs": [],
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "JkkXJ-MpKKSV",
        "outputId": "2c84d1f5-1a7c-4d00-b929-7b6d12d7695a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 276ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detector = MTCNN()\n",
        "image = cv2.imread('face-mask-detection/example_img/ex04.jpg', cv2.COLOR_BGR2RGB)\n",
        "faces = detector.detect_faces(image)\n",
        "for result in faces:\n",
        "    x, y, w, h = result['box']\n",
        "    x1, y1 = x + w, y + h\n",
        "    \n",
        "    # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "    # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\n",
        "    face = image[y:y1, x:x1]\n",
        "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "    face = cv2.resize(face, (224, 224))\n",
        "    face = img_to_array(face)\n",
        "    face = preprocess_input(face)\n",
        "    face = np.expand_dims(face, axis=0)    \n",
        "\n",
        "    # Membaca wajah dengan model\n",
        "    (mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "    # Menggunakan masker hijau, tidak bermasker merah\n",
        "    label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "    color = (0, 255, 0) if label == \"Bermasker\" else (0, 0, 255)\n",
        "\n",
        "\t\t# Probabilitas hasil deteksi\n",
        "    label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "    # Menampilkan hasil dengan label dan kotak\n",
        "    cv2.putText(image, label, (x, y - 10),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "    cv2.rectangle(image, (x, y), (x1, y1), color, 2)\n",
        "\n",
        "# Menampilkan output\n",
        "cv2.imshow(\" Hasil\", image)\n",
        "cv2.waitKey(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "njXIK_L3dVXx"
      },
      "source": [
        "## Pengujian Deteksi Perframe Capture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wdWN4xy-Nxd"
      },
      "outputs": [],
      "source": [
        "# Mengimport lib\n",
        "from mtcnn import MTCNN\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ajg0vfLNs6C"
      },
      "outputs": [],
      "source": [
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'Ketika selesai, klik disini atau pada video untuk berhenti dari demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        " \n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzQUeMGN5XQ"
      },
      "outputs": [],
      "source": [
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        " \n",
        "    return image_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETwcarZEN7Np"
      },
      "outputs": [],
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "\n",
        "# Menginisialisasi MTCNN\n",
        "detector = MTCNN()\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "while True:\n",
        "  js_reply = take_photo(label_html, img_data)\n",
        "  if not js_reply:\n",
        "    break\n",
        "    \n",
        "  image = js_reply_to_image(js_reply)\n",
        "\n",
        "\t# Mengambil frame dari aliran video berulir dan \n",
        "  # ukurannya maksimum lebar 400 pixel\n",
        "  frame = image\n",
        "  v=True\n",
        "  if v == True:\n",
        "\n",
        "    frame = imutils.resize(frame, width=400)\n",
        "\n",
        "    faces = detector.detect_faces(frame)\n",
        "    faces_list = []\n",
        "    preds = []\n",
        "    for result in faces:\n",
        "      x, y, w, h = result['box']\n",
        "      x1, y1 = x + w, y + h\n",
        "\n",
        "      # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "      # dan mengubah ukurannya menjadi 224x224, dan lalu pre-prosess\n",
        "      face = image[y:y1, x:x1]\n",
        "      face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "      face = cv2.resize(face, (224, 224))\n",
        "      face = img_to_array(face)\n",
        "      face = preprocess_input(face)\n",
        "      face = np.expand_dims(face, axis=0)\n",
        "     \n",
        "      faces_list.append(face)\n",
        "      \n",
        "      if len(faces_list) > 0:\n",
        "        preds = model.predict(faces_list)\n",
        "    \n",
        "      # Mendeteksi bermasker atau tidak\n",
        "      for pred in preds:\n",
        "        (mask, withoutMask) = pred\n",
        "\n",
        "\t  \t# Menggunakan masker hijau, tidak bermasker merah\n",
        "      label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "      color = (0, 255, 0) if label == \"Bermasker\" else (0, 0, 255)\n",
        "\n",
        "\t  \t# Probabilitas pada label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t  \t# Menampilkan hasil dengan label dan kotak dari frame\n",
        "      frame=cv2.putText(frame, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      frame=cv2.rectangle(frame, (x, y), (x1, y1), color, 2)\n",
        "\n",
        "\t# Menampilkan ouput\n",
        "      cv2_imshow(frame)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ziSC8bK9zR"
      },
      "source": [
        "## Pengujian Streaming Video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqoA42eicSdG"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from mtcnn import MTCNN\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Id8nYgQTDmw"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXrV4AMILGfb"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9whfB0urLIpj"
      },
      "outputs": [],
      "source": [
        "# Memulai streaming video dari webcam\n",
        "video_stream()\n",
        "# Label untuk video\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "count = 0 \n",
        "# Menginisialisasi MTCNN\n",
        "detector = MTCNN()\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Mengkonvert JS response ke OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    \n",
        "    # Membuat transparan overlay untuk bounding box\n",
        "    bbox_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    faces = detector.detect_faces(img)\n",
        "    faces_list = []\n",
        "    preds = []\n",
        "\n",
        "    for result in faces:\n",
        "      x, y, w, h = result['box']\n",
        "      x1, y1 = x + w, y + h\n",
        "      \n",
        "      # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "      # dan mengubah ukurannya menjadi 224x224, dan lalu pre-prosess\n",
        "      face = image[y:y1, x:x1]\n",
        "      face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "      face = cv2.resize(face, (224, 224))\n",
        "      face = img_to_array(face)\n",
        "      face = preprocess_input(face)\n",
        "      face = np.expand_dims(face, axis=0)\n",
        "     \n",
        "      faces_list.append(face)\n",
        "      \n",
        "      if len(faces_list) > 0:\n",
        "        preds = model.predict(faces_list)\n",
        "    \n",
        "      # Mendeteksi bermasker atau tidak\n",
        "      for pred in preds:\n",
        "        (mask, withoutMask) = pred\n",
        "\n",
        "\t\t  # Menggunakan masker hijau, tidak bermasker merah\n",
        "      label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "      color = (0, 255, 0) if label == \"Bermasker\" else (255, 0, 0)\n",
        "\n",
        "\t\t  # Probabilitas pada label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t  # Menampilkan hasil dengan label dan kotak dari frame\n",
        "      bbox_array=cv2.putText(bbox_array, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      bbox_array=cv2.rectangle(bbox_array, (x, y), (x1, y1), color, 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    # Mengkonversi overlay bbox ke dalam bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        " \n",
        "    # Mengupdate bbox ke pada frame selanjutnya untuk mendapat overlay\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "njXIK_L3dVXx"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
