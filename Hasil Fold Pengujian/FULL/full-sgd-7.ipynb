{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Face Mask Detection**\n\nProgram Machine Learning untuk mendeteksi penggunaan masker. Program dibuat menggunakan metode CNN dengan arsitektur VGG16Net dan MTCNN untuk face detection.\n","metadata":{"id":"44JRqlK3IyFv"}},{"cell_type":"markdown","source":"## Mengimpor Libraries yang dibutuhkan\n","metadata":{"id":"_BvjSZ30YhWh"}},{"cell_type":"code","source":"!git clone https://github.com/georgejusen/Skripsi","metadata":{"id":"JS_Cauc_X-kq","outputId":"f59d6bf1-7445-46eb-9df7-ed8e7ac63f78","execution":{"iopub.status.busy":"2023-11-26T05:18:39.882927Z","iopub.execute_input":"2023-11-26T05:18:39.883213Z","iopub.status.idle":"2023-11-26T05:18:47.893590Z","shell.execute_reply.started":"2023-11-26T05:18:39.883185Z","shell.execute_reply":"2023-11-26T05:18:47.892696Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Skripsi'...\nremote: Enumerating objects: 3930, done.\u001b[K\nremote: Counting objects: 100% (174/174), done.\u001b[K\nremote: Compressing objects: 100% (87/87), done.\u001b[K\nremote: Total 3930 (delta 92), reused 167 (delta 85), pack-reused 3756\u001b[K\nReceiving objects: 100% (3930/3930), 169.55 MiB | 46.72 MiB/s, done.\nResolving deltas: 100% (94/94), done.\nUpdating files: 100% (5831/5831), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install imutils","metadata":{"execution":{"iopub.status.busy":"2023-11-26T05:18:47.896012Z","iopub.execute_input":"2023-11-26T05:18:47.896464Z","iopub.status.idle":"2023-11-26T05:19:02.974608Z","shell.execute_reply.started":"2023-11-26T05:18:47.896422Z","shell.execute_reply":"2023-11-26T05:19:02.973454Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25837 sha256=2d5b5f25bdbd85272067041ffb96f5143fc7c3bc76d69b294b10711c99c071c4\n  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport pathlib\nimport numpy as np\nimport argparse\nimport os\nimport itertools\n\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ntf.test.gpu_device_name()\n\n!nvidia-smi\n\n\n","metadata":{"id":"W3Za-KOKf-Ot","outputId":"2956dbda-41c2-4764-9057-0537bf1359e3","execution":{"iopub.status.busy":"2023-11-26T05:19:02.975963Z","iopub.execute_input":"2023-11-26T05:19:02.976233Z","iopub.status.idle":"2023-11-26T05:19:19.523213Z","shell.execute_reply.started":"2023-11-26T05:19:02.976208Z","shell.execute_reply":"2023-11-26T05:19:19.522153Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Num GPUs Available:  2\nSun Nov 26 05:19:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0    26W /  70W |    308MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   45C    P0    26W /  70W |    308MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing Datasets\n","metadata":{"id":"ZU5Fqk2vLELg"}},{"cell_type":"code","source":"# Inisialisasi nilai Initial Learning Rate, berapa banyak Epoch pelatihan, dan Batch Size\nimport sys\nfrom PIL import Image\nimport os\nINIT_LR = 1e-4\nEPOCHS = 100\nBS = 32\nlipatan = 7\n\n# Mengambil gambar dari dataset directory, kemudian inisialisasi data dan class gambar\nprint(\"Menginput gambar...\")\n\n# imagePaths = list(paths.list_images('dataset'))\nimagePaths = list(paths.list_images(\"/kaggle/working/Skripsi/dataset\"))\ndata = []\nlabels = []\n\n# Melakukan perulangan pada image paths\nfor imagePath in imagePaths:\n\n    # Mengekstrak class label dari filename\n    label = imagePath.split(os.path.sep)[-2]\n    # Memuat input gambar (224x224) dan melakukan proses\n    image = load_img(imagePath, target_size=(224, 224))\n    image = img_to_array(image)\n    image = preprocess_input(image)\n\n    # Mengupdate data dan labels lists, berurutan\n    data.append(image)\n    labels.append(label)\n\n# Mengkonversi data dan label ke dalam NumPy Arrays\ndata = np.array(data, dtype=\"float32\")\nlabels = np.array(labels)\n\n# Melakukan one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\nprint(\"Input gambar berhasil\")","metadata":{"id":"D48VICs21pna","outputId":"8e886982-d0fd-4a27-f228-c21ee11c6921","execution":{"iopub.status.busy":"2023-11-26T05:19:19.525908Z","iopub.execute_input":"2023-11-26T05:19:19.527032Z","iopub.status.idle":"2023-11-26T05:19:29.827295Z","shell.execute_reply.started":"2023-11-26T05:19:19.527001Z","shell.execute_reply":"2023-11-26T05:19:29.826398Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Menginput gambar...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input gambar berhasil\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Membuat objek ImageDataGenerator dan Data Augmentation\n","metadata":{"id":"aF2vfrgni3tq"}},{"cell_type":"code","source":"# Mempartisi data ke dalam pelatihan dan pengujian ( 75% : 25% )\n'''\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n    test_size=0.25, stratify=labels, random_state=42)\n '''\n\n\n# Membentuk training image generator untuk data augmentation\naug = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\")","metadata":{"id":"tRBrygye5yvc","execution":{"iopub.status.busy":"2023-11-26T05:19:29.828707Z","iopub.execute_input":"2023-11-26T05:19:29.829134Z","iopub.status.idle":"2023-11-26T05:19:29.835214Z","shell.execute_reply.started":"2023-11-26T05:19:29.829084Z","shell.execute_reply":"2023-11-26T05:19:29.834304Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Membuat Model Jaringan CNN yang sudah dipelajari sebelumnya (pre-trained convnets)\n","metadata":{"id":"bHPD753f5205"}},{"cell_type":"code","source":"# Arsitektur jaringan VGG16Net\nbaseModel = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False,\n    input_tensor=Input(shape=(224, 224, 3)))","metadata":{"id":"FaDceTqP6HLr","execution":{"iopub.status.busy":"2023-11-26T05:19:29.836697Z","iopub.execute_input":"2023-11-26T05:19:29.837032Z","iopub.status.idle":"2023-11-26T05:19:30.948452Z","shell.execute_reply.started":"2023-11-26T05:19:29.836998Z","shell.execute_reply":"2023-11-26T05:19:30.947542Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Feature Extraction\n","metadata":{"id":"NfSLr2q2LWRZ"}},{"cell_type":"code","source":"baseModel.trainable = False\nbaseModel.summary()","metadata":{"id":"Z0U2qYnO6KYW","execution":{"iopub.status.busy":"2023-11-26T05:19:30.949788Z","iopub.execute_input":"2023-11-26T05:19:30.950087Z","iopub.status.idle":"2023-11-26T05:19:30.999477Z","shell.execute_reply.started":"2023-11-26T05:19:30.950062Z","shell.execute_reply":"2023-11-26T05:19:30.998665Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"vgg16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n=================================================================\nTotal params: 14714688 (56.13 MB)\nTrainable params: 0 (0.00 Byte)\nNon-trainable params: 14714688 (56.13 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tahap Pembuatan Model\n","metadata":{"id":"ENpg9P6Q6SpL"}},{"cell_type":"code","source":"# Membentuk bagian head dari model yang akan ditempatkan pada base model\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(128, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n\n# Menempatkan head model pada base model\nmodel = Model(inputs=baseModel.input, outputs=headModel)\nmodel.summary()\n\n","metadata":{"id":"NR5gCBjqXxkX","execution":{"iopub.status.busy":"2023-11-26T05:19:31.000573Z","iopub.execute_input":"2023-11-26T05:19:31.000893Z","iopub.status.idle":"2023-11-26T05:19:31.105667Z","shell.execute_reply.started":"2023-11-26T05:19:31.000860Z","shell.execute_reply":"2023-11-26T05:19:31.104765Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n average_pooling2d (Average  (None, 1, 1, 512)         0         \n Pooling2D)                                                      \n                                                                 \n flatten (Flatten)           (None, 512)               0         \n                                                                 \n dense (Dense)               (None, 128)               65664     \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 2)                 258       \n                                                                 \n=================================================================\nTotal params: 14780610 (56.38 MB)\nTrainable params: 65922 (257.51 KB)\nNon-trainable params: 14714688 (56.13 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=lipatan, shuffle=True)\n\nfold_accuracies = []\n\n# Perulangan pada seluruh base model\nfold = 1\nfor train_index, test_index in kf.split(data):\n    print(\"Mengkompilasi model...\")\n    print(\"fold\", fold)\n\n    train_data, train_labels = data[train_index], labels[train_index]\n    test_data, test_labels = data[test_index], labels[test_index]\n\n    opt = tf.keras.optimizers.legacy.SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\n    print(\"Training head model...\")\n\n    H = model.fit(\n        aug.flow(train_data, train_labels, batch_size=BS),\n        steps_per_epoch=len(train_data) // BS,\n        validation_data=(test_data, test_labels),\n        validation_steps=len(test_data) // BS,\n        epochs=EPOCHS)\n\n    loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n    fold_accuracies.append(accuracy)\n\n    fold += 1\n\n# Print fold accuracies\nfor fold, accuracy in enumerate(fold_accuracies, start=1):\n    print(f\"Fold {fold} - Accuracy: {accuracy:.4f}\")\n","metadata":{"id":"WXEyXB5NTU13","execution":{"iopub.status.busy":"2023-11-26T05:19:31.106829Z","iopub.execute_input":"2023-11-26T05:19:31.107098Z","iopub.status.idle":"2023-11-26T12:20:47.059375Z","shell.execute_reply.started":"2023-11-26T05:19:31.107074Z","shell.execute_reply":"2023-11-26T12:20:47.058414Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Mengkompilasi model...\nfold 1\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 55s 439ms/step - loss: 0.7971 - accuracy: 0.4464 - val_loss: 0.7421 - val_accuracy: 0.2974\nEpoch 2/100\n102/102 [==============================] - 39s 380ms/step - loss: 0.7931 - accuracy: 0.4399 - val_loss: 0.7373 - val_accuracy: 0.2865\nEpoch 3/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7843 - accuracy: 0.4577 - val_loss: 0.7334 - val_accuracy: 0.2774\nEpoch 4/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.7747 - accuracy: 0.4580 - val_loss: 0.7305 - val_accuracy: 0.2664\nEpoch 5/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7775 - accuracy: 0.4559 - val_loss: 0.7278 - val_accuracy: 0.2573\nEpoch 6/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.7727 - accuracy: 0.4537 - val_loss: 0.7255 - val_accuracy: 0.2628\nEpoch 7/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.7816 - accuracy: 0.4507 - val_loss: 0.7234 - val_accuracy: 0.2701\nEpoch 8/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.7650 - accuracy: 0.4682 - val_loss: 0.7216 - val_accuracy: 0.2774\nEpoch 9/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.7656 - accuracy: 0.4688 - val_loss: 0.7198 - val_accuracy: 0.2865\nEpoch 10/100\n102/102 [==============================] - 36s 358ms/step - loss: 0.7612 - accuracy: 0.4636 - val_loss: 0.7182 - val_accuracy: 0.2883\nEpoch 11/100\n102/102 [==============================] - 37s 365ms/step - loss: 0.7654 - accuracy: 0.4519 - val_loss: 0.7167 - val_accuracy: 0.2993\nEpoch 12/100\n102/102 [==============================] - 37s 366ms/step - loss: 0.7633 - accuracy: 0.4587 - val_loss: 0.7152 - val_accuracy: 0.3047\nEpoch 13/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.7543 - accuracy: 0.4734 - val_loss: 0.7139 - val_accuracy: 0.3120\nEpoch 14/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.7589 - accuracy: 0.4605 - val_loss: 0.7125 - val_accuracy: 0.3248\nEpoch 15/100\n102/102 [==============================] - 36s 358ms/step - loss: 0.7494 - accuracy: 0.4762 - val_loss: 0.7112 - val_accuracy: 0.3358\nEpoch 16/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.7564 - accuracy: 0.4691 - val_loss: 0.7100 - val_accuracy: 0.3449\nEpoch 17/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7513 - accuracy: 0.4811 - val_loss: 0.7088 - val_accuracy: 0.3595\nEpoch 18/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.7531 - accuracy: 0.4636 - val_loss: 0.7076 - val_accuracy: 0.3723\nEpoch 19/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7525 - accuracy: 0.4709 - val_loss: 0.7065 - val_accuracy: 0.3850\nEpoch 20/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.7443 - accuracy: 0.4799 - val_loss: 0.7054 - val_accuracy: 0.4033\nEpoch 21/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.7428 - accuracy: 0.4836 - val_loss: 0.7043 - val_accuracy: 0.4179\nEpoch 22/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.7425 - accuracy: 0.4829 - val_loss: 0.7032 - val_accuracy: 0.4252\nEpoch 23/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.7420 - accuracy: 0.4802 - val_loss: 0.7021 - val_accuracy: 0.4343\nEpoch 24/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.7351 - accuracy: 0.4922 - val_loss: 0.7010 - val_accuracy: 0.4398\nEpoch 25/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.7396 - accuracy: 0.4857 - val_loss: 0.7000 - val_accuracy: 0.4562\nEpoch 26/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.7344 - accuracy: 0.4875 - val_loss: 0.6990 - val_accuracy: 0.4690\nEpoch 27/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7388 - accuracy: 0.4909 - val_loss: 0.6980 - val_accuracy: 0.4854\nEpoch 28/100\n102/102 [==============================] - 37s 362ms/step - loss: 0.7349 - accuracy: 0.4891 - val_loss: 0.6970 - val_accuracy: 0.4945\nEpoch 29/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.7353 - accuracy: 0.4872 - val_loss: 0.6959 - val_accuracy: 0.5164\nEpoch 30/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.7295 - accuracy: 0.5088 - val_loss: 0.6950 - val_accuracy: 0.5219\nEpoch 31/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.7378 - accuracy: 0.4869 - val_loss: 0.6941 - val_accuracy: 0.5292\nEpoch 32/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7301 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5383\nEpoch 33/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7304 - accuracy: 0.4946 - val_loss: 0.6922 - val_accuracy: 0.5493\nEpoch 34/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.7277 - accuracy: 0.4928 - val_loss: 0.6913 - val_accuracy: 0.5602\nEpoch 35/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.7275 - accuracy: 0.5066 - val_loss: 0.6904 - val_accuracy: 0.5730\nEpoch 36/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.7236 - accuracy: 0.5118 - val_loss: 0.6895 - val_accuracy: 0.5894\nEpoch 37/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.7265 - accuracy: 0.4989 - val_loss: 0.6887 - val_accuracy: 0.5949\nEpoch 38/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.7213 - accuracy: 0.5119 - val_loss: 0.6879 - val_accuracy: 0.6058\nEpoch 39/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.7257 - accuracy: 0.5051 - val_loss: 0.6871 - val_accuracy: 0.6223\nEpoch 40/100\n102/102 [==============================] - 37s 365ms/step - loss: 0.7178 - accuracy: 0.5238 - val_loss: 0.6863 - val_accuracy: 0.6314\nEpoch 41/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7229 - accuracy: 0.5008 - val_loss: 0.6855 - val_accuracy: 0.6423\nEpoch 42/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7111 - accuracy: 0.5334 - val_loss: 0.6847 - val_accuracy: 0.6478\nEpoch 43/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.7198 - accuracy: 0.5155 - val_loss: 0.6840 - val_accuracy: 0.6624\nEpoch 44/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.7164 - accuracy: 0.5125 - val_loss: 0.6832 - val_accuracy: 0.6734\nEpoch 45/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.7152 - accuracy: 0.5263 - val_loss: 0.6825 - val_accuracy: 0.6825\nEpoch 46/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.7147 - accuracy: 0.5312 - val_loss: 0.6817 - val_accuracy: 0.6934\nEpoch 47/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.7107 - accuracy: 0.5235 - val_loss: 0.6810 - val_accuracy: 0.6971\nEpoch 48/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7086 - accuracy: 0.5389 - val_loss: 0.6803 - val_accuracy: 0.7026\nEpoch 49/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7118 - accuracy: 0.5358 - val_loss: 0.6796 - val_accuracy: 0.7099\nEpoch 50/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.7079 - accuracy: 0.5398 - val_loss: 0.6789 - val_accuracy: 0.7135\nEpoch 51/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.7070 - accuracy: 0.5312 - val_loss: 0.6782 - val_accuracy: 0.7208\nEpoch 52/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.7108 - accuracy: 0.5284 - val_loss: 0.6776 - val_accuracy: 0.7299\nEpoch 53/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.7067 - accuracy: 0.5352 - val_loss: 0.6769 - val_accuracy: 0.7336\nEpoch 54/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.7091 - accuracy: 0.5291 - val_loss: 0.6762 - val_accuracy: 0.7354\nEpoch 55/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.7026 - accuracy: 0.5530 - val_loss: 0.6756 - val_accuracy: 0.7391\nEpoch 56/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.7040 - accuracy: 0.5432 - val_loss: 0.6749 - val_accuracy: 0.7445\nEpoch 57/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.7011 - accuracy: 0.5555 - val_loss: 0.6743 - val_accuracy: 0.7518\nEpoch 58/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.7015 - accuracy: 0.5484 - val_loss: 0.6736 - val_accuracy: 0.7591\nEpoch 59/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.7042 - accuracy: 0.5386 - val_loss: 0.6729 - val_accuracy: 0.7664\nEpoch 60/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.7013 - accuracy: 0.5524 - val_loss: 0.6722 - val_accuracy: 0.7701\nEpoch 61/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.7069 - accuracy: 0.5269 - val_loss: 0.6715 - val_accuracy: 0.7719\nEpoch 62/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6955 - accuracy: 0.5644 - val_loss: 0.6708 - val_accuracy: 0.7737\nEpoch 63/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6938 - accuracy: 0.5567 - val_loss: 0.6701 - val_accuracy: 0.7755\nEpoch 64/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6924 - accuracy: 0.5684 - val_loss: 0.6694 - val_accuracy: 0.7774\nEpoch 65/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6967 - accuracy: 0.5613 - val_loss: 0.6687 - val_accuracy: 0.7810\nEpoch 66/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6967 - accuracy: 0.5503 - val_loss: 0.6680 - val_accuracy: 0.7865\nEpoch 67/100\n102/102 [==============================] - 37s 366ms/step - loss: 0.6920 - accuracy: 0.5632 - val_loss: 0.6672 - val_accuracy: 0.7901\nEpoch 68/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.6948 - accuracy: 0.5583 - val_loss: 0.6665 - val_accuracy: 0.7920\nEpoch 69/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6907 - accuracy: 0.5687 - val_loss: 0.6658 - val_accuracy: 0.7938\nEpoch 70/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.6913 - accuracy: 0.5687 - val_loss: 0.6651 - val_accuracy: 0.7938\nEpoch 71/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6891 - accuracy: 0.5862 - val_loss: 0.6645 - val_accuracy: 0.7974\nEpoch 72/100\n102/102 [==============================] - 37s 362ms/step - loss: 0.6894 - accuracy: 0.5721 - val_loss: 0.6638 - val_accuracy: 0.7974\nEpoch 73/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6897 - accuracy: 0.5761 - val_loss: 0.6631 - val_accuracy: 0.7974\nEpoch 74/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.6848 - accuracy: 0.5865 - val_loss: 0.6625 - val_accuracy: 0.7956\nEpoch 75/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6888 - accuracy: 0.5798 - val_loss: 0.6619 - val_accuracy: 0.7993\nEpoch 76/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.6868 - accuracy: 0.5835 - val_loss: 0.6613 - val_accuracy: 0.8029\nEpoch 77/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6855 - accuracy: 0.5813 - val_loss: 0.6607 - val_accuracy: 0.8029\nEpoch 78/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6848 - accuracy: 0.5816 - val_loss: 0.6601 - val_accuracy: 0.8066\nEpoch 79/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6842 - accuracy: 0.5930 - val_loss: 0.6595 - val_accuracy: 0.8066\nEpoch 80/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.6823 - accuracy: 0.5902 - val_loss: 0.6589 - val_accuracy: 0.8102\nEpoch 81/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6838 - accuracy: 0.5828 - val_loss: 0.6583 - val_accuracy: 0.8102\nEpoch 82/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6814 - accuracy: 0.5893 - val_loss: 0.6577 - val_accuracy: 0.8084\nEpoch 83/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.6828 - accuracy: 0.5862 - val_loss: 0.6571 - val_accuracy: 0.8139\nEpoch 84/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.6828 - accuracy: 0.5856 - val_loss: 0.6566 - val_accuracy: 0.8139\nEpoch 85/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6795 - accuracy: 0.5982 - val_loss: 0.6560 - val_accuracy: 0.8139\nEpoch 86/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.6771 - accuracy: 0.5976 - val_loss: 0.6554 - val_accuracy: 0.8120\nEpoch 87/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.6774 - accuracy: 0.6028 - val_loss: 0.6548 - val_accuracy: 0.8157\nEpoch 88/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6768 - accuracy: 0.5994 - val_loss: 0.6542 - val_accuracy: 0.8120\nEpoch 89/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.6760 - accuracy: 0.6044 - val_loss: 0.6536 - val_accuracy: 0.8120\nEpoch 90/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6801 - accuracy: 0.5942 - val_loss: 0.6530 - val_accuracy: 0.8120\nEpoch 91/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6730 - accuracy: 0.6139 - val_loss: 0.6524 - val_accuracy: 0.8175\nEpoch 92/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6760 - accuracy: 0.6127 - val_loss: 0.6517 - val_accuracy: 0.8175\nEpoch 93/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6743 - accuracy: 0.6102 - val_loss: 0.6510 - val_accuracy: 0.8212\nEpoch 94/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.6736 - accuracy: 0.6277 - val_loss: 0.6503 - val_accuracy: 0.8193\nEpoch 95/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6695 - accuracy: 0.6087 - val_loss: 0.6495 - val_accuracy: 0.8212\nEpoch 96/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6712 - accuracy: 0.6268 - val_loss: 0.6488 - val_accuracy: 0.8230\nEpoch 97/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6709 - accuracy: 0.6117 - val_loss: 0.6480 - val_accuracy: 0.8212\nEpoch 98/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6690 - accuracy: 0.6287 - val_loss: 0.6473 - val_accuracy: 0.8266\nEpoch 99/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.6677 - accuracy: 0.6277 - val_loss: 0.6465 - val_accuracy: 0.8285\nEpoch 100/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6648 - accuracy: 0.6400 - val_loss: 0.6458 - val_accuracy: 0.8321\nMengkompilasi model...\nfold 2\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6683 - accuracy: 0.6204 - val_loss: 0.6434 - val_accuracy: 0.8631\nEpoch 2/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6718 - accuracy: 0.6111 - val_loss: 0.6428 - val_accuracy: 0.8686\nEpoch 3/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6669 - accuracy: 0.6247 - val_loss: 0.6422 - val_accuracy: 0.8686\nEpoch 4/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6637 - accuracy: 0.6357 - val_loss: 0.6416 - val_accuracy: 0.8686\nEpoch 5/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6695 - accuracy: 0.6231 - val_loss: 0.6410 - val_accuracy: 0.8741\nEpoch 6/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6628 - accuracy: 0.6526 - val_loss: 0.6405 - val_accuracy: 0.8777\nEpoch 7/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6594 - accuracy: 0.6505 - val_loss: 0.6399 - val_accuracy: 0.8814\nEpoch 8/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6626 - accuracy: 0.6449 - val_loss: 0.6393 - val_accuracy: 0.8814\nEpoch 9/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6641 - accuracy: 0.6354 - val_loss: 0.6388 - val_accuracy: 0.8869\nEpoch 10/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.6588 - accuracy: 0.6446 - val_loss: 0.6382 - val_accuracy: 0.8869\nEpoch 11/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6578 - accuracy: 0.6499 - val_loss: 0.6376 - val_accuracy: 0.8869\nEpoch 12/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6578 - accuracy: 0.6471 - val_loss: 0.6370 - val_accuracy: 0.8905\nEpoch 13/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6619 - accuracy: 0.6431 - val_loss: 0.6364 - val_accuracy: 0.8905\nEpoch 14/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6565 - accuracy: 0.6591 - val_loss: 0.6358 - val_accuracy: 0.8905\nEpoch 15/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6614 - accuracy: 0.6379 - val_loss: 0.6352 - val_accuracy: 0.8905\nEpoch 16/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6567 - accuracy: 0.6615 - val_loss: 0.6347 - val_accuracy: 0.8923\nEpoch 17/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.6556 - accuracy: 0.6585 - val_loss: 0.6341 - val_accuracy: 0.8923\nEpoch 18/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6578 - accuracy: 0.6517 - val_loss: 0.6336 - val_accuracy: 0.8923\nEpoch 19/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6544 - accuracy: 0.6625 - val_loss: 0.6330 - val_accuracy: 0.8905\nEpoch 20/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6547 - accuracy: 0.6686 - val_loss: 0.6324 - val_accuracy: 0.8905\nEpoch 21/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6514 - accuracy: 0.6741 - val_loss: 0.6319 - val_accuracy: 0.8905\nEpoch 22/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6530 - accuracy: 0.6680 - val_loss: 0.6313 - val_accuracy: 0.8905\nEpoch 23/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6509 - accuracy: 0.6766 - val_loss: 0.6307 - val_accuracy: 0.8923\nEpoch 24/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6475 - accuracy: 0.6803 - val_loss: 0.6302 - val_accuracy: 0.8942\nEpoch 25/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6526 - accuracy: 0.6585 - val_loss: 0.6296 - val_accuracy: 0.8942\nEpoch 26/100\n102/102 [==============================] - 37s 365ms/step - loss: 0.6477 - accuracy: 0.6809 - val_loss: 0.6290 - val_accuracy: 0.8942\nEpoch 27/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.6530 - accuracy: 0.6674 - val_loss: 0.6285 - val_accuracy: 0.8960\nEpoch 28/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6511 - accuracy: 0.6710 - val_loss: 0.6280 - val_accuracy: 0.8960\nEpoch 29/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6477 - accuracy: 0.6821 - val_loss: 0.6274 - val_accuracy: 0.8960\nEpoch 30/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6513 - accuracy: 0.6702 - val_loss: 0.6269 - val_accuracy: 0.8960\nEpoch 31/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.6439 - accuracy: 0.6929 - val_loss: 0.6264 - val_accuracy: 0.8978\nEpoch 32/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6504 - accuracy: 0.6671 - val_loss: 0.6259 - val_accuracy: 0.8978\nEpoch 33/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.6477 - accuracy: 0.6815 - val_loss: 0.6252 - val_accuracy: 0.8978\nEpoch 34/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6502 - accuracy: 0.6729 - val_loss: 0.6247 - val_accuracy: 0.8960\nEpoch 35/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6452 - accuracy: 0.6837 - val_loss: 0.6242 - val_accuracy: 0.8978\nEpoch 36/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6420 - accuracy: 0.6960 - val_loss: 0.6237 - val_accuracy: 0.8978\nEpoch 37/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6467 - accuracy: 0.6874 - val_loss: 0.6231 - val_accuracy: 0.8978\nEpoch 38/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.6474 - accuracy: 0.6809 - val_loss: 0.6226 - val_accuracy: 0.8978\nEpoch 39/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6462 - accuracy: 0.6846 - val_loss: 0.6221 - val_accuracy: 0.8978\nEpoch 40/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6464 - accuracy: 0.6800 - val_loss: 0.6216 - val_accuracy: 0.8978\nEpoch 41/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6447 - accuracy: 0.6883 - val_loss: 0.6210 - val_accuracy: 0.8978\nEpoch 42/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6385 - accuracy: 0.7052 - val_loss: 0.6205 - val_accuracy: 0.8978\nEpoch 43/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6442 - accuracy: 0.6923 - val_loss: 0.6200 - val_accuracy: 0.8978\nEpoch 44/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6442 - accuracy: 0.6929 - val_loss: 0.6194 - val_accuracy: 0.8996\nEpoch 45/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6449 - accuracy: 0.6815 - val_loss: 0.6190 - val_accuracy: 0.8978\nEpoch 46/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6373 - accuracy: 0.7058 - val_loss: 0.6184 - val_accuracy: 0.8996\nEpoch 47/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6414 - accuracy: 0.6901 - val_loss: 0.6178 - val_accuracy: 0.9015\nEpoch 48/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6397 - accuracy: 0.6920 - val_loss: 0.6173 - val_accuracy: 0.9015\nEpoch 49/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.6414 - accuracy: 0.6963 - val_loss: 0.6169 - val_accuracy: 0.9015\nEpoch 50/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6384 - accuracy: 0.7080 - val_loss: 0.6163 - val_accuracy: 0.9033\nEpoch 51/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.6385 - accuracy: 0.7067 - val_loss: 0.6158 - val_accuracy: 0.9051\nEpoch 52/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6377 - accuracy: 0.7003 - val_loss: 0.6153 - val_accuracy: 0.9051\nEpoch 53/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6368 - accuracy: 0.7172 - val_loss: 0.6148 - val_accuracy: 0.9051\nEpoch 54/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.6354 - accuracy: 0.7163 - val_loss: 0.6142 - val_accuracy: 0.9069\nEpoch 55/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6410 - accuracy: 0.6846 - val_loss: 0.6137 - val_accuracy: 0.9069\nEpoch 56/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6341 - accuracy: 0.7175 - val_loss: 0.6131 - val_accuracy: 0.9069\nEpoch 57/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6357 - accuracy: 0.7138 - val_loss: 0.6126 - val_accuracy: 0.9069\nEpoch 58/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6307 - accuracy: 0.7261 - val_loss: 0.6120 - val_accuracy: 0.9069\nEpoch 59/100\n102/102 [==============================] - 37s 363ms/step - loss: 0.6379 - accuracy: 0.7043 - val_loss: 0.6115 - val_accuracy: 0.9069\nEpoch 60/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6359 - accuracy: 0.7058 - val_loss: 0.6109 - val_accuracy: 0.9069\nEpoch 61/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6302 - accuracy: 0.7230 - val_loss: 0.6104 - val_accuracy: 0.9069\nEpoch 62/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.6300 - accuracy: 0.7301 - val_loss: 0.6098 - val_accuracy: 0.9069\nEpoch 63/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6289 - accuracy: 0.7332 - val_loss: 0.6092 - val_accuracy: 0.9051\nEpoch 64/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.6294 - accuracy: 0.7313 - val_loss: 0.6086 - val_accuracy: 0.9033\nEpoch 65/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.6319 - accuracy: 0.7233 - val_loss: 0.6082 - val_accuracy: 0.9051\nEpoch 66/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6314 - accuracy: 0.7178 - val_loss: 0.6076 - val_accuracy: 0.9051\nEpoch 67/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6335 - accuracy: 0.7123 - val_loss: 0.6071 - val_accuracy: 0.9051\nEpoch 68/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.6281 - accuracy: 0.7261 - val_loss: 0.6066 - val_accuracy: 0.9051\nEpoch 69/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.6304 - accuracy: 0.7243 - val_loss: 0.6060 - val_accuracy: 0.9051\nEpoch 70/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6285 - accuracy: 0.7175 - val_loss: 0.6055 - val_accuracy: 0.9051\nEpoch 71/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6264 - accuracy: 0.7270 - val_loss: 0.6051 - val_accuracy: 0.9051\nEpoch 72/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6258 - accuracy: 0.7310 - val_loss: 0.6045 - val_accuracy: 0.9051\nEpoch 73/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.6275 - accuracy: 0.7283 - val_loss: 0.6040 - val_accuracy: 0.9051\nEpoch 74/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6256 - accuracy: 0.7298 - val_loss: 0.6034 - val_accuracy: 0.9051\nEpoch 75/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6264 - accuracy: 0.7344 - val_loss: 0.6029 - val_accuracy: 0.9051\nEpoch 76/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6270 - accuracy: 0.7246 - val_loss: 0.6023 - val_accuracy: 0.9051\nEpoch 77/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.6240 - accuracy: 0.7418 - val_loss: 0.6019 - val_accuracy: 0.9051\nEpoch 78/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6252 - accuracy: 0.7344 - val_loss: 0.6013 - val_accuracy: 0.9051\nEpoch 79/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6227 - accuracy: 0.7445 - val_loss: 0.6007 - val_accuracy: 0.9069\nEpoch 80/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6216 - accuracy: 0.7479 - val_loss: 0.6002 - val_accuracy: 0.9069\nEpoch 81/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6231 - accuracy: 0.7366 - val_loss: 0.5996 - val_accuracy: 0.9069\nEpoch 82/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6233 - accuracy: 0.7415 - val_loss: 0.5991 - val_accuracy: 0.9069\nEpoch 83/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.6219 - accuracy: 0.7412 - val_loss: 0.5986 - val_accuracy: 0.9069\nEpoch 84/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6221 - accuracy: 0.7362 - val_loss: 0.5981 - val_accuracy: 0.9069\nEpoch 85/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6203 - accuracy: 0.7393 - val_loss: 0.5976 - val_accuracy: 0.9069\nEpoch 86/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6228 - accuracy: 0.7381 - val_loss: 0.5971 - val_accuracy: 0.9069\nEpoch 87/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.6204 - accuracy: 0.7384 - val_loss: 0.5965 - val_accuracy: 0.9069\nEpoch 88/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6207 - accuracy: 0.7418 - val_loss: 0.5960 - val_accuracy: 0.9069\nEpoch 89/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6175 - accuracy: 0.7519 - val_loss: 0.5954 - val_accuracy: 0.9069\nEpoch 90/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6172 - accuracy: 0.7596 - val_loss: 0.5949 - val_accuracy: 0.9069\nEpoch 91/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6191 - accuracy: 0.7510 - val_loss: 0.5944 - val_accuracy: 0.9069\nEpoch 92/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6174 - accuracy: 0.7488 - val_loss: 0.5938 - val_accuracy: 0.9069\nEpoch 93/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6157 - accuracy: 0.7547 - val_loss: 0.5933 - val_accuracy: 0.9069\nEpoch 94/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6148 - accuracy: 0.7565 - val_loss: 0.5927 - val_accuracy: 0.9069\nEpoch 95/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6147 - accuracy: 0.7593 - val_loss: 0.5922 - val_accuracy: 0.9088\nEpoch 96/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6154 - accuracy: 0.7547 - val_loss: 0.5916 - val_accuracy: 0.9088\nEpoch 97/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6171 - accuracy: 0.7409 - val_loss: 0.5911 - val_accuracy: 0.9088\nEpoch 98/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.6108 - accuracy: 0.7676 - val_loss: 0.5906 - val_accuracy: 0.9088\nEpoch 99/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6148 - accuracy: 0.7645 - val_loss: 0.5901 - val_accuracy: 0.9088\nEpoch 100/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6135 - accuracy: 0.7584 - val_loss: 0.5896 - val_accuracy: 0.9069\nMengkompilasi model...\nfold 3\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6127 - accuracy: 0.7562 - val_loss: 0.5889 - val_accuracy: 0.9033\nEpoch 2/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6118 - accuracy: 0.7596 - val_loss: 0.5885 - val_accuracy: 0.9033\nEpoch 3/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6144 - accuracy: 0.7596 - val_loss: 0.5880 - val_accuracy: 0.9033\nEpoch 4/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.6097 - accuracy: 0.7667 - val_loss: 0.5875 - val_accuracy: 0.8996\nEpoch 5/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6097 - accuracy: 0.7636 - val_loss: 0.5870 - val_accuracy: 0.9015\nEpoch 6/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6116 - accuracy: 0.7571 - val_loss: 0.5865 - val_accuracy: 0.9015\nEpoch 7/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6151 - accuracy: 0.7427 - val_loss: 0.5861 - val_accuracy: 0.9015\nEpoch 8/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6096 - accuracy: 0.7737 - val_loss: 0.5856 - val_accuracy: 0.9015\nEpoch 9/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6086 - accuracy: 0.7590 - val_loss: 0.5851 - val_accuracy: 0.9051\nEpoch 10/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6109 - accuracy: 0.7645 - val_loss: 0.5846 - val_accuracy: 0.9033\nEpoch 11/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.6098 - accuracy: 0.7679 - val_loss: 0.5841 - val_accuracy: 0.9033\nEpoch 12/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6108 - accuracy: 0.7688 - val_loss: 0.5836 - val_accuracy: 0.9033\nEpoch 13/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.6085 - accuracy: 0.7664 - val_loss: 0.5830 - val_accuracy: 0.9051\nEpoch 14/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6068 - accuracy: 0.7648 - val_loss: 0.5825 - val_accuracy: 0.9051\nEpoch 15/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.6034 - accuracy: 0.7676 - val_loss: 0.5819 - val_accuracy: 0.9051\nEpoch 16/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6062 - accuracy: 0.7658 - val_loss: 0.5814 - val_accuracy: 0.9051\nEpoch 17/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6061 - accuracy: 0.7661 - val_loss: 0.5809 - val_accuracy: 0.9051\nEpoch 18/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6050 - accuracy: 0.7741 - val_loss: 0.5804 - val_accuracy: 0.9051\nEpoch 19/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6037 - accuracy: 0.7802 - val_loss: 0.5799 - val_accuracy: 0.9051\nEpoch 20/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6044 - accuracy: 0.7667 - val_loss: 0.5793 - val_accuracy: 0.9051\nEpoch 21/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.6012 - accuracy: 0.7793 - val_loss: 0.5788 - val_accuracy: 0.9051\nEpoch 22/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.6034 - accuracy: 0.7731 - val_loss: 0.5782 - val_accuracy: 0.9051\nEpoch 23/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.6056 - accuracy: 0.7713 - val_loss: 0.5777 - val_accuracy: 0.9069\nEpoch 24/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.6026 - accuracy: 0.7777 - val_loss: 0.5772 - val_accuracy: 0.9069\nEpoch 25/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.6026 - accuracy: 0.7814 - val_loss: 0.5766 - val_accuracy: 0.9069\nEpoch 26/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5990 - accuracy: 0.7808 - val_loss: 0.5761 - val_accuracy: 0.9069\nEpoch 27/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.6004 - accuracy: 0.7799 - val_loss: 0.5756 - val_accuracy: 0.9069\nEpoch 28/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5990 - accuracy: 0.7796 - val_loss: 0.5751 - val_accuracy: 0.9069\nEpoch 29/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.6000 - accuracy: 0.7820 - val_loss: 0.5746 - val_accuracy: 0.9069\nEpoch 30/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5967 - accuracy: 0.7897 - val_loss: 0.5741 - val_accuracy: 0.9069\nEpoch 31/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.6012 - accuracy: 0.7750 - val_loss: 0.5736 - val_accuracy: 0.9069\nEpoch 32/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5990 - accuracy: 0.7857 - val_loss: 0.5730 - val_accuracy: 0.9069\nEpoch 33/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5988 - accuracy: 0.7799 - val_loss: 0.5725 - val_accuracy: 0.9069\nEpoch 34/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5970 - accuracy: 0.7882 - val_loss: 0.5719 - val_accuracy: 0.9069\nEpoch 35/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.5965 - accuracy: 0.7836 - val_loss: 0.5714 - val_accuracy: 0.9069\nEpoch 36/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.6007 - accuracy: 0.7799 - val_loss: 0.5708 - val_accuracy: 0.9069\nEpoch 37/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5955 - accuracy: 0.7833 - val_loss: 0.5703 - val_accuracy: 0.9069\nEpoch 38/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5981 - accuracy: 0.7805 - val_loss: 0.5698 - val_accuracy: 0.9069\nEpoch 39/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5972 - accuracy: 0.7827 - val_loss: 0.5693 - val_accuracy: 0.9069\nEpoch 40/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5956 - accuracy: 0.7784 - val_loss: 0.5688 - val_accuracy: 0.9069\nEpoch 41/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5960 - accuracy: 0.7885 - val_loss: 0.5682 - val_accuracy: 0.9069\nEpoch 42/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5961 - accuracy: 0.7827 - val_loss: 0.5677 - val_accuracy: 0.9069\nEpoch 43/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5944 - accuracy: 0.7864 - val_loss: 0.5672 - val_accuracy: 0.9069\nEpoch 44/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5939 - accuracy: 0.7910 - val_loss: 0.5666 - val_accuracy: 0.9069\nEpoch 45/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5907 - accuracy: 0.7937 - val_loss: 0.5661 - val_accuracy: 0.9051\nEpoch 46/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.5946 - accuracy: 0.7876 - val_loss: 0.5656 - val_accuracy: 0.9051\nEpoch 47/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.5916 - accuracy: 0.7943 - val_loss: 0.5650 - val_accuracy: 0.9033\nEpoch 48/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5914 - accuracy: 0.7953 - val_loss: 0.5645 - val_accuracy: 0.9069\nEpoch 49/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.5919 - accuracy: 0.7965 - val_loss: 0.5640 - val_accuracy: 0.9069\nEpoch 50/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5898 - accuracy: 0.7873 - val_loss: 0.5635 - val_accuracy: 0.9051\nEpoch 51/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5883 - accuracy: 0.7903 - val_loss: 0.5629 - val_accuracy: 0.9069\nEpoch 52/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5893 - accuracy: 0.7894 - val_loss: 0.5623 - val_accuracy: 0.9033\nEpoch 53/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5860 - accuracy: 0.8060 - val_loss: 0.5618 - val_accuracy: 0.9051\nEpoch 54/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5863 - accuracy: 0.8103 - val_loss: 0.5613 - val_accuracy: 0.9051\nEpoch 55/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5857 - accuracy: 0.8106 - val_loss: 0.5608 - val_accuracy: 0.9088\nEpoch 56/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5902 - accuracy: 0.7913 - val_loss: 0.5603 - val_accuracy: 0.9088\nEpoch 57/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5844 - accuracy: 0.8036 - val_loss: 0.5598 - val_accuracy: 0.9088\nEpoch 58/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5814 - accuracy: 0.7925 - val_loss: 0.5592 - val_accuracy: 0.9088\nEpoch 59/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5872 - accuracy: 0.7996 - val_loss: 0.5587 - val_accuracy: 0.9106\nEpoch 60/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5835 - accuracy: 0.7947 - val_loss: 0.5581 - val_accuracy: 0.9106\nEpoch 61/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5878 - accuracy: 0.7888 - val_loss: 0.5576 - val_accuracy: 0.9106\nEpoch 62/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5869 - accuracy: 0.7993 - val_loss: 0.5571 - val_accuracy: 0.9106\nEpoch 63/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5876 - accuracy: 0.7937 - val_loss: 0.5566 - val_accuracy: 0.9106\nEpoch 64/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5843 - accuracy: 0.7999 - val_loss: 0.5561 - val_accuracy: 0.9106\nEpoch 65/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5853 - accuracy: 0.7971 - val_loss: 0.5556 - val_accuracy: 0.9106\nEpoch 66/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5802 - accuracy: 0.8048 - val_loss: 0.5551 - val_accuracy: 0.9106\nEpoch 67/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5799 - accuracy: 0.8023 - val_loss: 0.5545 - val_accuracy: 0.9124\nEpoch 68/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5809 - accuracy: 0.8060 - val_loss: 0.5540 - val_accuracy: 0.9142\nEpoch 69/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5794 - accuracy: 0.8094 - val_loss: 0.5535 - val_accuracy: 0.9142\nEpoch 70/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5828 - accuracy: 0.7947 - val_loss: 0.5529 - val_accuracy: 0.9124\nEpoch 71/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5777 - accuracy: 0.8091 - val_loss: 0.5523 - val_accuracy: 0.9124\nEpoch 72/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5804 - accuracy: 0.7980 - val_loss: 0.5517 - val_accuracy: 0.9124\nEpoch 73/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5819 - accuracy: 0.8051 - val_loss: 0.5512 - val_accuracy: 0.9106\nEpoch 74/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5761 - accuracy: 0.8214 - val_loss: 0.5507 - val_accuracy: 0.9106\nEpoch 75/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5789 - accuracy: 0.8106 - val_loss: 0.5502 - val_accuracy: 0.9124\nEpoch 76/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5737 - accuracy: 0.8211 - val_loss: 0.5496 - val_accuracy: 0.9124\nEpoch 77/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5775 - accuracy: 0.8085 - val_loss: 0.5490 - val_accuracy: 0.9106\nEpoch 78/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5744 - accuracy: 0.8159 - val_loss: 0.5484 - val_accuracy: 0.9106\nEpoch 79/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5766 - accuracy: 0.8165 - val_loss: 0.5479 - val_accuracy: 0.9124\nEpoch 80/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5781 - accuracy: 0.8103 - val_loss: 0.5474 - val_accuracy: 0.9124\nEpoch 81/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5771 - accuracy: 0.8079 - val_loss: 0.5469 - val_accuracy: 0.9124\nEpoch 82/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5703 - accuracy: 0.8171 - val_loss: 0.5463 - val_accuracy: 0.9124\nEpoch 83/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5757 - accuracy: 0.8168 - val_loss: 0.5458 - val_accuracy: 0.9124\nEpoch 84/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5735 - accuracy: 0.8143 - val_loss: 0.5453 - val_accuracy: 0.9124\nEpoch 85/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.5753 - accuracy: 0.8026 - val_loss: 0.5448 - val_accuracy: 0.9124\nEpoch 86/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5725 - accuracy: 0.8174 - val_loss: 0.5442 - val_accuracy: 0.9124\nEpoch 87/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.5753 - accuracy: 0.8128 - val_loss: 0.5437 - val_accuracy: 0.9124\nEpoch 88/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5708 - accuracy: 0.8140 - val_loss: 0.5432 - val_accuracy: 0.9124\nEpoch 89/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5722 - accuracy: 0.8177 - val_loss: 0.5426 - val_accuracy: 0.9124\nEpoch 90/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5685 - accuracy: 0.8156 - val_loss: 0.5421 - val_accuracy: 0.9106\nEpoch 91/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5724 - accuracy: 0.8146 - val_loss: 0.5415 - val_accuracy: 0.9124\nEpoch 92/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5725 - accuracy: 0.8162 - val_loss: 0.5410 - val_accuracy: 0.9124\nEpoch 93/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5675 - accuracy: 0.8189 - val_loss: 0.5404 - val_accuracy: 0.9124\nEpoch 94/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5697 - accuracy: 0.8257 - val_loss: 0.5398 - val_accuracy: 0.9124\nEpoch 95/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5694 - accuracy: 0.8189 - val_loss: 0.5393 - val_accuracy: 0.9124\nEpoch 96/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5742 - accuracy: 0.8017 - val_loss: 0.5388 - val_accuracy: 0.9142\nEpoch 97/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5651 - accuracy: 0.8306 - val_loss: 0.5382 - val_accuracy: 0.9124\nEpoch 98/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5658 - accuracy: 0.8248 - val_loss: 0.5376 - val_accuracy: 0.9124\nEpoch 99/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5667 - accuracy: 0.8217 - val_loss: 0.5371 - val_accuracy: 0.9124\nEpoch 100/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5690 - accuracy: 0.8143 - val_loss: 0.5366 - val_accuracy: 0.9124\nMengkompilasi model...\nfold 4\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 37s 356ms/step - loss: 0.5701 - accuracy: 0.8011 - val_loss: 0.5308 - val_accuracy: 0.9234\nEpoch 2/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5686 - accuracy: 0.8229 - val_loss: 0.5302 - val_accuracy: 0.9234\nEpoch 3/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5662 - accuracy: 0.8186 - val_loss: 0.5296 - val_accuracy: 0.9234\nEpoch 4/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5653 - accuracy: 0.8239 - val_loss: 0.5290 - val_accuracy: 0.9252\nEpoch 5/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5644 - accuracy: 0.8297 - val_loss: 0.5285 - val_accuracy: 0.9234\nEpoch 6/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5607 - accuracy: 0.8272 - val_loss: 0.5280 - val_accuracy: 0.9234\nEpoch 7/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5663 - accuracy: 0.8137 - val_loss: 0.5275 - val_accuracy: 0.9215\nEpoch 8/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5610 - accuracy: 0.8285 - val_loss: 0.5269 - val_accuracy: 0.9234\nEpoch 9/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5632 - accuracy: 0.8275 - val_loss: 0.5263 - val_accuracy: 0.9234\nEpoch 10/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5615 - accuracy: 0.8211 - val_loss: 0.5258 - val_accuracy: 0.9215\nEpoch 11/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5629 - accuracy: 0.8239 - val_loss: 0.5252 - val_accuracy: 0.9234\nEpoch 12/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5653 - accuracy: 0.8192 - val_loss: 0.5246 - val_accuracy: 0.9252\nEpoch 13/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.5590 - accuracy: 0.8386 - val_loss: 0.5240 - val_accuracy: 0.9252\nEpoch 14/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5562 - accuracy: 0.8285 - val_loss: 0.5234 - val_accuracy: 0.9252\nEpoch 15/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5577 - accuracy: 0.8137 - val_loss: 0.5228 - val_accuracy: 0.9252\nEpoch 16/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5585 - accuracy: 0.8300 - val_loss: 0.5223 - val_accuracy: 0.9252\nEpoch 17/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5545 - accuracy: 0.8362 - val_loss: 0.5217 - val_accuracy: 0.9252\nEpoch 18/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5612 - accuracy: 0.8257 - val_loss: 0.5211 - val_accuracy: 0.9252\nEpoch 19/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5609 - accuracy: 0.8162 - val_loss: 0.5206 - val_accuracy: 0.9252\nEpoch 20/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5609 - accuracy: 0.8242 - val_loss: 0.5200 - val_accuracy: 0.9252\nEpoch 21/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5519 - accuracy: 0.8383 - val_loss: 0.5195 - val_accuracy: 0.9252\nEpoch 22/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5601 - accuracy: 0.8260 - val_loss: 0.5190 - val_accuracy: 0.9252\nEpoch 23/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5535 - accuracy: 0.8248 - val_loss: 0.5184 - val_accuracy: 0.9252\nEpoch 24/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5539 - accuracy: 0.8365 - val_loss: 0.5179 - val_accuracy: 0.9252\nEpoch 25/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5510 - accuracy: 0.8349 - val_loss: 0.5172 - val_accuracy: 0.9252\nEpoch 26/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5566 - accuracy: 0.8352 - val_loss: 0.5167 - val_accuracy: 0.9252\nEpoch 27/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5560 - accuracy: 0.8275 - val_loss: 0.5162 - val_accuracy: 0.9252\nEpoch 28/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5552 - accuracy: 0.8288 - val_loss: 0.5156 - val_accuracy: 0.9252\nEpoch 29/100\n102/102 [==============================] - 37s 360ms/step - loss: 0.5460 - accuracy: 0.8420 - val_loss: 0.5150 - val_accuracy: 0.9252\nEpoch 30/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5578 - accuracy: 0.8146 - val_loss: 0.5144 - val_accuracy: 0.9252\nEpoch 31/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5509 - accuracy: 0.8239 - val_loss: 0.5138 - val_accuracy: 0.9252\nEpoch 32/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5543 - accuracy: 0.8282 - val_loss: 0.5132 - val_accuracy: 0.9252\nEpoch 33/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5524 - accuracy: 0.8288 - val_loss: 0.5127 - val_accuracy: 0.9252\nEpoch 34/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5496 - accuracy: 0.8358 - val_loss: 0.5122 - val_accuracy: 0.9252\nEpoch 35/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5520 - accuracy: 0.8377 - val_loss: 0.5116 - val_accuracy: 0.9252\nEpoch 36/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5495 - accuracy: 0.8460 - val_loss: 0.5110 - val_accuracy: 0.9252\nEpoch 37/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5495 - accuracy: 0.8275 - val_loss: 0.5105 - val_accuracy: 0.9252\nEpoch 38/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5474 - accuracy: 0.8358 - val_loss: 0.5099 - val_accuracy: 0.9270\nEpoch 39/100\n102/102 [==============================] - 35s 343ms/step - loss: 0.5483 - accuracy: 0.8309 - val_loss: 0.5094 - val_accuracy: 0.9270\nEpoch 40/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5507 - accuracy: 0.8248 - val_loss: 0.5088 - val_accuracy: 0.9270\nEpoch 41/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5493 - accuracy: 0.8352 - val_loss: 0.5083 - val_accuracy: 0.9270\nEpoch 42/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5471 - accuracy: 0.8346 - val_loss: 0.5077 - val_accuracy: 0.9270\nEpoch 43/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5510 - accuracy: 0.8291 - val_loss: 0.5072 - val_accuracy: 0.9270\nEpoch 44/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5439 - accuracy: 0.8383 - val_loss: 0.5066 - val_accuracy: 0.9270\nEpoch 45/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5464 - accuracy: 0.8343 - val_loss: 0.5060 - val_accuracy: 0.9270\nEpoch 46/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5449 - accuracy: 0.8337 - val_loss: 0.5055 - val_accuracy: 0.9270\nEpoch 47/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5450 - accuracy: 0.8303 - val_loss: 0.5049 - val_accuracy: 0.9270\nEpoch 48/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5387 - accuracy: 0.8469 - val_loss: 0.5044 - val_accuracy: 0.9270\nEpoch 49/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5438 - accuracy: 0.8383 - val_loss: 0.5038 - val_accuracy: 0.9270\nEpoch 50/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5392 - accuracy: 0.8411 - val_loss: 0.5033 - val_accuracy: 0.9270\nEpoch 51/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5429 - accuracy: 0.8309 - val_loss: 0.5028 - val_accuracy: 0.9270\nEpoch 52/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5407 - accuracy: 0.8429 - val_loss: 0.5021 - val_accuracy: 0.9270\nEpoch 53/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.5411 - accuracy: 0.8386 - val_loss: 0.5015 - val_accuracy: 0.9270\nEpoch 54/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5385 - accuracy: 0.8377 - val_loss: 0.5009 - val_accuracy: 0.9270\nEpoch 55/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5417 - accuracy: 0.8368 - val_loss: 0.5004 - val_accuracy: 0.9270\nEpoch 56/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.5396 - accuracy: 0.8389 - val_loss: 0.4998 - val_accuracy: 0.9270\nEpoch 57/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5399 - accuracy: 0.8340 - val_loss: 0.4993 - val_accuracy: 0.9270\nEpoch 58/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5411 - accuracy: 0.8355 - val_loss: 0.4987 - val_accuracy: 0.9288\nEpoch 59/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5364 - accuracy: 0.8500 - val_loss: 0.4981 - val_accuracy: 0.9270\nEpoch 60/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5383 - accuracy: 0.8334 - val_loss: 0.4976 - val_accuracy: 0.9288\nEpoch 61/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5407 - accuracy: 0.8340 - val_loss: 0.4970 - val_accuracy: 0.9288\nEpoch 62/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5403 - accuracy: 0.8355 - val_loss: 0.4965 - val_accuracy: 0.9288\nEpoch 63/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5355 - accuracy: 0.8472 - val_loss: 0.4958 - val_accuracy: 0.9288\nEpoch 64/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5384 - accuracy: 0.8451 - val_loss: 0.4953 - val_accuracy: 0.9288\nEpoch 65/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5390 - accuracy: 0.8383 - val_loss: 0.4948 - val_accuracy: 0.9288\nEpoch 66/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5393 - accuracy: 0.8398 - val_loss: 0.4942 - val_accuracy: 0.9288\nEpoch 67/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5361 - accuracy: 0.8414 - val_loss: 0.4937 - val_accuracy: 0.9288\nEpoch 68/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5323 - accuracy: 0.8475 - val_loss: 0.4930 - val_accuracy: 0.9288\nEpoch 69/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5333 - accuracy: 0.8466 - val_loss: 0.4924 - val_accuracy: 0.9288\nEpoch 70/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5294 - accuracy: 0.8528 - val_loss: 0.4918 - val_accuracy: 0.9288\nEpoch 71/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5342 - accuracy: 0.8540 - val_loss: 0.4913 - val_accuracy: 0.9288\nEpoch 72/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5319 - accuracy: 0.8466 - val_loss: 0.4907 - val_accuracy: 0.9288\nEpoch 73/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5344 - accuracy: 0.8414 - val_loss: 0.4901 - val_accuracy: 0.9288\nEpoch 74/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5322 - accuracy: 0.8423 - val_loss: 0.4896 - val_accuracy: 0.9288\nEpoch 75/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5319 - accuracy: 0.8411 - val_loss: 0.4890 - val_accuracy: 0.9288\nEpoch 76/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.5293 - accuracy: 0.8448 - val_loss: 0.4884 - val_accuracy: 0.9288\nEpoch 77/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.5293 - accuracy: 0.8484 - val_loss: 0.4879 - val_accuracy: 0.9288\nEpoch 78/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5339 - accuracy: 0.8352 - val_loss: 0.4873 - val_accuracy: 0.9288\nEpoch 79/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.5277 - accuracy: 0.8438 - val_loss: 0.4868 - val_accuracy: 0.9288\nEpoch 80/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5328 - accuracy: 0.8460 - val_loss: 0.4863 - val_accuracy: 0.9288\nEpoch 81/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5217 - accuracy: 0.8500 - val_loss: 0.4857 - val_accuracy: 0.9288\nEpoch 82/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5249 - accuracy: 0.8484 - val_loss: 0.4850 - val_accuracy: 0.9288\nEpoch 83/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5247 - accuracy: 0.8509 - val_loss: 0.4844 - val_accuracy: 0.9288\nEpoch 84/100\n102/102 [==============================] - 35s 342ms/step - loss: 0.5271 - accuracy: 0.8469 - val_loss: 0.4838 - val_accuracy: 0.9288\nEpoch 85/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5263 - accuracy: 0.8457 - val_loss: 0.4832 - val_accuracy: 0.9288\nEpoch 86/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5259 - accuracy: 0.8500 - val_loss: 0.4827 - val_accuracy: 0.9288\nEpoch 87/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5311 - accuracy: 0.8340 - val_loss: 0.4821 - val_accuracy: 0.9288\nEpoch 88/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5245 - accuracy: 0.8383 - val_loss: 0.4816 - val_accuracy: 0.9288\nEpoch 89/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5266 - accuracy: 0.8484 - val_loss: 0.4811 - val_accuracy: 0.9288\nEpoch 90/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5226 - accuracy: 0.8448 - val_loss: 0.4805 - val_accuracy: 0.9288\nEpoch 91/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5224 - accuracy: 0.8500 - val_loss: 0.4800 - val_accuracy: 0.9288\nEpoch 92/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5224 - accuracy: 0.8549 - val_loss: 0.4794 - val_accuracy: 0.9288\nEpoch 93/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5239 - accuracy: 0.8540 - val_loss: 0.4788 - val_accuracy: 0.9288\nEpoch 94/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5232 - accuracy: 0.8491 - val_loss: 0.4783 - val_accuracy: 0.9288\nEpoch 95/100\n102/102 [==============================] - 35s 343ms/step - loss: 0.5230 - accuracy: 0.8432 - val_loss: 0.4776 - val_accuracy: 0.9288\nEpoch 96/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5231 - accuracy: 0.8429 - val_loss: 0.4771 - val_accuracy: 0.9307\nEpoch 97/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5188 - accuracy: 0.8491 - val_loss: 0.4764 - val_accuracy: 0.9307\nEpoch 98/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5203 - accuracy: 0.8509 - val_loss: 0.4759 - val_accuracy: 0.9307\nEpoch 99/100\n102/102 [==============================] - 35s 343ms/step - loss: 0.5233 - accuracy: 0.8463 - val_loss: 0.4753 - val_accuracy: 0.9307\nEpoch 100/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5153 - accuracy: 0.8601 - val_loss: 0.4746 - val_accuracy: 0.9307\nMengkompilasi model...\nfold 5\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 43s 412ms/step - loss: 0.5147 - accuracy: 0.8553 - val_loss: 0.4842 - val_accuracy: 0.9141\nEpoch 2/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.5182 - accuracy: 0.8482 - val_loss: 0.4836 - val_accuracy: 0.9159\nEpoch 3/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5138 - accuracy: 0.8574 - val_loss: 0.4830 - val_accuracy: 0.9177\nEpoch 4/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5144 - accuracy: 0.8565 - val_loss: 0.4825 - val_accuracy: 0.9159\nEpoch 5/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.5135 - accuracy: 0.8639 - val_loss: 0.4819 - val_accuracy: 0.9177\nEpoch 6/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.5145 - accuracy: 0.8537 - val_loss: 0.4814 - val_accuracy: 0.9177\nEpoch 7/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5134 - accuracy: 0.8543 - val_loss: 0.4809 - val_accuracy: 0.9159\nEpoch 8/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5150 - accuracy: 0.8467 - val_loss: 0.4803 - val_accuracy: 0.9177\nEpoch 9/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.5141 - accuracy: 0.8626 - val_loss: 0.4797 - val_accuracy: 0.9177\nEpoch 10/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5113 - accuracy: 0.8549 - val_loss: 0.4791 - val_accuracy: 0.9196\nEpoch 11/100\n102/102 [==============================] - 35s 342ms/step - loss: 0.5106 - accuracy: 0.8556 - val_loss: 0.4785 - val_accuracy: 0.9196\nEpoch 12/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.5109 - accuracy: 0.8553 - val_loss: 0.4780 - val_accuracy: 0.9196\nEpoch 13/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5116 - accuracy: 0.8556 - val_loss: 0.4775 - val_accuracy: 0.9196\nEpoch 14/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5107 - accuracy: 0.8599 - val_loss: 0.4769 - val_accuracy: 0.9196\nEpoch 15/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5080 - accuracy: 0.8685 - val_loss: 0.4764 - val_accuracy: 0.9196\nEpoch 16/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5077 - accuracy: 0.8626 - val_loss: 0.4758 - val_accuracy: 0.9196\nEpoch 17/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5130 - accuracy: 0.8525 - val_loss: 0.4753 - val_accuracy: 0.9196\nEpoch 18/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5125 - accuracy: 0.8559 - val_loss: 0.4747 - val_accuracy: 0.9196\nEpoch 19/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.5063 - accuracy: 0.8589 - val_loss: 0.4742 - val_accuracy: 0.9196\nEpoch 20/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5108 - accuracy: 0.8629 - val_loss: 0.4737 - val_accuracy: 0.9177\nEpoch 21/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.5066 - accuracy: 0.8663 - val_loss: 0.4732 - val_accuracy: 0.9177\nEpoch 22/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5041 - accuracy: 0.8629 - val_loss: 0.4727 - val_accuracy: 0.9177\nEpoch 23/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5053 - accuracy: 0.8586 - val_loss: 0.4722 - val_accuracy: 0.9177\nEpoch 24/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5066 - accuracy: 0.8574 - val_loss: 0.4717 - val_accuracy: 0.9177\nEpoch 25/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5033 - accuracy: 0.8586 - val_loss: 0.4710 - val_accuracy: 0.9177\nEpoch 26/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5075 - accuracy: 0.8559 - val_loss: 0.4704 - val_accuracy: 0.9177\nEpoch 27/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5045 - accuracy: 0.8648 - val_loss: 0.4698 - val_accuracy: 0.9196\nEpoch 28/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5083 - accuracy: 0.8522 - val_loss: 0.4694 - val_accuracy: 0.9214\nEpoch 29/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.5048 - accuracy: 0.8497 - val_loss: 0.4689 - val_accuracy: 0.9214\nEpoch 30/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.5015 - accuracy: 0.8596 - val_loss: 0.4684 - val_accuracy: 0.9214\nEpoch 31/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.5046 - accuracy: 0.8611 - val_loss: 0.4679 - val_accuracy: 0.9214\nEpoch 32/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.5005 - accuracy: 0.8648 - val_loss: 0.4674 - val_accuracy: 0.9214\nEpoch 33/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4998 - accuracy: 0.8746 - val_loss: 0.4669 - val_accuracy: 0.9177\nEpoch 34/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5007 - accuracy: 0.8593 - val_loss: 0.4663 - val_accuracy: 0.9196\nEpoch 35/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4981 - accuracy: 0.8636 - val_loss: 0.4658 - val_accuracy: 0.9177\nEpoch 36/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4980 - accuracy: 0.8672 - val_loss: 0.4652 - val_accuracy: 0.9196\nEpoch 37/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4992 - accuracy: 0.8571 - val_loss: 0.4646 - val_accuracy: 0.9232\nEpoch 38/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.5029 - accuracy: 0.8559 - val_loss: 0.4640 - val_accuracy: 0.9232\nEpoch 39/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4985 - accuracy: 0.8639 - val_loss: 0.4635 - val_accuracy: 0.9232\nEpoch 40/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.5022 - accuracy: 0.8510 - val_loss: 0.4630 - val_accuracy: 0.9232\nEpoch 41/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.5000 - accuracy: 0.8636 - val_loss: 0.4624 - val_accuracy: 0.9232\nEpoch 42/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.5019 - accuracy: 0.8586 - val_loss: 0.4619 - val_accuracy: 0.9232\nEpoch 43/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4953 - accuracy: 0.8629 - val_loss: 0.4614 - val_accuracy: 0.9232\nEpoch 44/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4953 - accuracy: 0.8648 - val_loss: 0.4609 - val_accuracy: 0.9232\nEpoch 45/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4970 - accuracy: 0.8565 - val_loss: 0.4604 - val_accuracy: 0.9232\nEpoch 46/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4966 - accuracy: 0.8608 - val_loss: 0.4599 - val_accuracy: 0.9232\nEpoch 47/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4932 - accuracy: 0.8645 - val_loss: 0.4594 - val_accuracy: 0.9232\nEpoch 48/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4947 - accuracy: 0.8642 - val_loss: 0.4588 - val_accuracy: 0.9232\nEpoch 49/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4940 - accuracy: 0.8666 - val_loss: 0.4582 - val_accuracy: 0.9232\nEpoch 50/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4930 - accuracy: 0.8694 - val_loss: 0.4578 - val_accuracy: 0.9232\nEpoch 51/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4942 - accuracy: 0.8602 - val_loss: 0.4573 - val_accuracy: 0.9232\nEpoch 52/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4919 - accuracy: 0.8626 - val_loss: 0.4566 - val_accuracy: 0.9232\nEpoch 53/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4943 - accuracy: 0.8589 - val_loss: 0.4562 - val_accuracy: 0.9232\nEpoch 54/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4918 - accuracy: 0.8651 - val_loss: 0.4557 - val_accuracy: 0.9232\nEpoch 55/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4912 - accuracy: 0.8654 - val_loss: 0.4551 - val_accuracy: 0.9232\nEpoch 56/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4910 - accuracy: 0.8596 - val_loss: 0.4545 - val_accuracy: 0.9232\nEpoch 57/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4874 - accuracy: 0.8728 - val_loss: 0.4540 - val_accuracy: 0.9232\nEpoch 58/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4913 - accuracy: 0.8608 - val_loss: 0.4535 - val_accuracy: 0.9232\nEpoch 59/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4831 - accuracy: 0.8792 - val_loss: 0.4529 - val_accuracy: 0.9232\nEpoch 60/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4942 - accuracy: 0.8645 - val_loss: 0.4524 - val_accuracy: 0.9250\nEpoch 61/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4878 - accuracy: 0.8642 - val_loss: 0.4519 - val_accuracy: 0.9232\nEpoch 62/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4867 - accuracy: 0.8636 - val_loss: 0.4513 - val_accuracy: 0.9250\nEpoch 63/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4891 - accuracy: 0.8623 - val_loss: 0.4508 - val_accuracy: 0.9232\nEpoch 64/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4847 - accuracy: 0.8685 - val_loss: 0.4502 - val_accuracy: 0.9232\nEpoch 65/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4830 - accuracy: 0.8688 - val_loss: 0.4497 - val_accuracy: 0.9232\nEpoch 66/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4853 - accuracy: 0.8611 - val_loss: 0.4492 - val_accuracy: 0.9232\nEpoch 67/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4847 - accuracy: 0.8712 - val_loss: 0.4487 - val_accuracy: 0.9232\nEpoch 68/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4896 - accuracy: 0.8645 - val_loss: 0.4481 - val_accuracy: 0.9232\nEpoch 69/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4868 - accuracy: 0.8629 - val_loss: 0.4476 - val_accuracy: 0.9250\nEpoch 70/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4874 - accuracy: 0.8623 - val_loss: 0.4471 - val_accuracy: 0.9269\nEpoch 71/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4858 - accuracy: 0.8593 - val_loss: 0.4467 - val_accuracy: 0.9232\nEpoch 72/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4840 - accuracy: 0.8672 - val_loss: 0.4461 - val_accuracy: 0.9250\nEpoch 73/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4842 - accuracy: 0.8605 - val_loss: 0.4456 - val_accuracy: 0.9269\nEpoch 74/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4850 - accuracy: 0.8565 - val_loss: 0.4452 - val_accuracy: 0.9250\nEpoch 75/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4805 - accuracy: 0.8749 - val_loss: 0.4447 - val_accuracy: 0.9250\nEpoch 76/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4851 - accuracy: 0.8669 - val_loss: 0.4442 - val_accuracy: 0.9250\nEpoch 77/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4827 - accuracy: 0.8605 - val_loss: 0.4438 - val_accuracy: 0.9250\nEpoch 78/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4855 - accuracy: 0.8586 - val_loss: 0.4433 - val_accuracy: 0.9269\nEpoch 79/100\n102/102 [==============================] - 35s 343ms/step - loss: 0.4796 - accuracy: 0.8629 - val_loss: 0.4428 - val_accuracy: 0.9250\nEpoch 80/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4803 - accuracy: 0.8722 - val_loss: 0.4422 - val_accuracy: 0.9269\nEpoch 81/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4812 - accuracy: 0.8654 - val_loss: 0.4418 - val_accuracy: 0.9250\nEpoch 82/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4818 - accuracy: 0.8669 - val_loss: 0.4412 - val_accuracy: 0.9269\nEpoch 83/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4800 - accuracy: 0.8639 - val_loss: 0.4409 - val_accuracy: 0.9250\nEpoch 84/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4779 - accuracy: 0.8712 - val_loss: 0.4402 - val_accuracy: 0.9269\nEpoch 85/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4772 - accuracy: 0.8722 - val_loss: 0.4397 - val_accuracy: 0.9250\nEpoch 86/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4751 - accuracy: 0.8666 - val_loss: 0.4391 - val_accuracy: 0.9287\nEpoch 87/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4766 - accuracy: 0.8669 - val_loss: 0.4385 - val_accuracy: 0.9287\nEpoch 88/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4765 - accuracy: 0.8691 - val_loss: 0.4381 - val_accuracy: 0.9287\nEpoch 89/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4753 - accuracy: 0.8672 - val_loss: 0.4374 - val_accuracy: 0.9269\nEpoch 90/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4777 - accuracy: 0.8642 - val_loss: 0.4370 - val_accuracy: 0.9269\nEpoch 91/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4769 - accuracy: 0.8675 - val_loss: 0.4365 - val_accuracy: 0.9287\nEpoch 92/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.4742 - accuracy: 0.8645 - val_loss: 0.4361 - val_accuracy: 0.9287\nEpoch 93/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4761 - accuracy: 0.8651 - val_loss: 0.4355 - val_accuracy: 0.9269\nEpoch 94/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4738 - accuracy: 0.8685 - val_loss: 0.4349 - val_accuracy: 0.9269\nEpoch 95/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4749 - accuracy: 0.8715 - val_loss: 0.4343 - val_accuracy: 0.9269\nEpoch 96/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4769 - accuracy: 0.8632 - val_loss: 0.4339 - val_accuracy: 0.9269\nEpoch 97/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4703 - accuracy: 0.8774 - val_loss: 0.4333 - val_accuracy: 0.9269\nEpoch 98/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4750 - accuracy: 0.8639 - val_loss: 0.4329 - val_accuracy: 0.9287\nEpoch 99/100\n102/102 [==============================] - 35s 343ms/step - loss: 0.4678 - accuracy: 0.8777 - val_loss: 0.4322 - val_accuracy: 0.9269\nEpoch 100/100\n102/102 [==============================] - 35s 341ms/step - loss: 0.4679 - accuracy: 0.8749 - val_loss: 0.4317 - val_accuracy: 0.9269\nMengkompilasi model...\nfold 6\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 37s 355ms/step - loss: 0.4740 - accuracy: 0.8694 - val_loss: 0.4203 - val_accuracy: 0.9250\nEpoch 2/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4758 - accuracy: 0.8636 - val_loss: 0.4198 - val_accuracy: 0.9250\nEpoch 3/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4651 - accuracy: 0.8823 - val_loss: 0.4192 - val_accuracy: 0.9250\nEpoch 4/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4691 - accuracy: 0.8734 - val_loss: 0.4187 - val_accuracy: 0.9250\nEpoch 5/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4723 - accuracy: 0.8691 - val_loss: 0.4182 - val_accuracy: 0.9250\nEpoch 6/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4712 - accuracy: 0.8614 - val_loss: 0.4176 - val_accuracy: 0.9250\nEpoch 7/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4683 - accuracy: 0.8758 - val_loss: 0.4171 - val_accuracy: 0.9250\nEpoch 8/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4688 - accuracy: 0.8740 - val_loss: 0.4166 - val_accuracy: 0.9287\nEpoch 9/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4656 - accuracy: 0.8752 - val_loss: 0.4160 - val_accuracy: 0.9250\nEpoch 10/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4621 - accuracy: 0.8755 - val_loss: 0.4154 - val_accuracy: 0.9250\nEpoch 11/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4715 - accuracy: 0.8752 - val_loss: 0.4149 - val_accuracy: 0.9287\nEpoch 12/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4727 - accuracy: 0.8605 - val_loss: 0.4144 - val_accuracy: 0.9250\nEpoch 13/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4687 - accuracy: 0.8691 - val_loss: 0.4139 - val_accuracy: 0.9287\nEpoch 14/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4658 - accuracy: 0.8672 - val_loss: 0.4134 - val_accuracy: 0.9287\nEpoch 15/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4685 - accuracy: 0.8715 - val_loss: 0.4128 - val_accuracy: 0.9305\nEpoch 16/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4635 - accuracy: 0.8758 - val_loss: 0.4123 - val_accuracy: 0.9305\nEpoch 17/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4622 - accuracy: 0.8786 - val_loss: 0.4118 - val_accuracy: 0.9305\nEpoch 18/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4631 - accuracy: 0.8811 - val_loss: 0.4112 - val_accuracy: 0.9287\nEpoch 19/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4656 - accuracy: 0.8737 - val_loss: 0.4107 - val_accuracy: 0.9305\nEpoch 20/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4627 - accuracy: 0.8706 - val_loss: 0.4101 - val_accuracy: 0.9305\nEpoch 21/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4633 - accuracy: 0.8789 - val_loss: 0.4096 - val_accuracy: 0.9305\nEpoch 22/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4565 - accuracy: 0.8829 - val_loss: 0.4090 - val_accuracy: 0.9287\nEpoch 23/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4640 - accuracy: 0.8749 - val_loss: 0.4084 - val_accuracy: 0.9287\nEpoch 24/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4641 - accuracy: 0.8734 - val_loss: 0.4079 - val_accuracy: 0.9287\nEpoch 25/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4595 - accuracy: 0.8719 - val_loss: 0.4074 - val_accuracy: 0.9287\nEpoch 26/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4586 - accuracy: 0.8789 - val_loss: 0.4069 - val_accuracy: 0.9287\nEpoch 27/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4600 - accuracy: 0.8758 - val_loss: 0.4063 - val_accuracy: 0.9287\nEpoch 28/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4636 - accuracy: 0.8688 - val_loss: 0.4058 - val_accuracy: 0.9287\nEpoch 29/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4537 - accuracy: 0.8820 - val_loss: 0.4052 - val_accuracy: 0.9305\nEpoch 30/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4592 - accuracy: 0.8697 - val_loss: 0.4047 - val_accuracy: 0.9287\nEpoch 31/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4606 - accuracy: 0.8771 - val_loss: 0.4041 - val_accuracy: 0.9305\nEpoch 32/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4611 - accuracy: 0.8645 - val_loss: 0.4036 - val_accuracy: 0.9305\nEpoch 33/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4546 - accuracy: 0.8712 - val_loss: 0.4030 - val_accuracy: 0.9305\nEpoch 34/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4557 - accuracy: 0.8777 - val_loss: 0.4025 - val_accuracy: 0.9305\nEpoch 35/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4541 - accuracy: 0.8820 - val_loss: 0.4019 - val_accuracy: 0.9305\nEpoch 36/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4526 - accuracy: 0.8780 - val_loss: 0.4013 - val_accuracy: 0.9287\nEpoch 37/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4586 - accuracy: 0.8749 - val_loss: 0.4006 - val_accuracy: 0.9305\nEpoch 38/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.4599 - accuracy: 0.8703 - val_loss: 0.4001 - val_accuracy: 0.9287\nEpoch 39/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4502 - accuracy: 0.8795 - val_loss: 0.3995 - val_accuracy: 0.9287\nEpoch 40/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4474 - accuracy: 0.8823 - val_loss: 0.3988 - val_accuracy: 0.9287\nEpoch 41/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4548 - accuracy: 0.8758 - val_loss: 0.3982 - val_accuracy: 0.9287\nEpoch 42/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4504 - accuracy: 0.8817 - val_loss: 0.3976 - val_accuracy: 0.9287\nEpoch 43/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4520 - accuracy: 0.8743 - val_loss: 0.3970 - val_accuracy: 0.9287\nEpoch 44/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4506 - accuracy: 0.8731 - val_loss: 0.3964 - val_accuracy: 0.9287\nEpoch 45/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4495 - accuracy: 0.8854 - val_loss: 0.3956 - val_accuracy: 0.9287\nEpoch 46/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4465 - accuracy: 0.8866 - val_loss: 0.3949 - val_accuracy: 0.9287\nEpoch 47/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4464 - accuracy: 0.8854 - val_loss: 0.3943 - val_accuracy: 0.9287\nEpoch 48/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4498 - accuracy: 0.8725 - val_loss: 0.3937 - val_accuracy: 0.9287\nEpoch 49/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4500 - accuracy: 0.8774 - val_loss: 0.3932 - val_accuracy: 0.9287\nEpoch 50/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4498 - accuracy: 0.8783 - val_loss: 0.3926 - val_accuracy: 0.9287\nEpoch 51/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4499 - accuracy: 0.8771 - val_loss: 0.3921 - val_accuracy: 0.9287\nEpoch 52/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4460 - accuracy: 0.8808 - val_loss: 0.3914 - val_accuracy: 0.9287\nEpoch 53/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4459 - accuracy: 0.8789 - val_loss: 0.3909 - val_accuracy: 0.9287\nEpoch 54/100\n102/102 [==============================] - 38s 371ms/step - loss: 0.4484 - accuracy: 0.8752 - val_loss: 0.3904 - val_accuracy: 0.9305\nEpoch 55/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4466 - accuracy: 0.8734 - val_loss: 0.3899 - val_accuracy: 0.9305\nEpoch 56/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4409 - accuracy: 0.8832 - val_loss: 0.3894 - val_accuracy: 0.9287\nEpoch 57/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4486 - accuracy: 0.8645 - val_loss: 0.3888 - val_accuracy: 0.9305\nEpoch 58/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4439 - accuracy: 0.8780 - val_loss: 0.3884 - val_accuracy: 0.9305\nEpoch 59/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4447 - accuracy: 0.8792 - val_loss: 0.3879 - val_accuracy: 0.9305\nEpoch 60/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4386 - accuracy: 0.8848 - val_loss: 0.3874 - val_accuracy: 0.9305\nEpoch 61/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4449 - accuracy: 0.8755 - val_loss: 0.3869 - val_accuracy: 0.9305\nEpoch 62/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4406 - accuracy: 0.8863 - val_loss: 0.3864 - val_accuracy: 0.9305\nEpoch 63/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4411 - accuracy: 0.8820 - val_loss: 0.3859 - val_accuracy: 0.9287\nEpoch 64/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4409 - accuracy: 0.8783 - val_loss: 0.3855 - val_accuracy: 0.9305\nEpoch 65/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4418 - accuracy: 0.8749 - val_loss: 0.3850 - val_accuracy: 0.9287\nEpoch 66/100\n102/102 [==============================] - 36s 357ms/step - loss: 0.4414 - accuracy: 0.8749 - val_loss: 0.3845 - val_accuracy: 0.9305\nEpoch 67/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4427 - accuracy: 0.8792 - val_loss: 0.3840 - val_accuracy: 0.9305\nEpoch 68/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4396 - accuracy: 0.8829 - val_loss: 0.3835 - val_accuracy: 0.9305\nEpoch 69/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4408 - accuracy: 0.8706 - val_loss: 0.3831 - val_accuracy: 0.9305\nEpoch 70/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.4435 - accuracy: 0.8786 - val_loss: 0.3826 - val_accuracy: 0.9305\nEpoch 71/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4404 - accuracy: 0.8872 - val_loss: 0.3821 - val_accuracy: 0.9305\nEpoch 72/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4360 - accuracy: 0.8749 - val_loss: 0.3816 - val_accuracy: 0.9305\nEpoch 73/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4398 - accuracy: 0.8823 - val_loss: 0.3812 - val_accuracy: 0.9305\nEpoch 74/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4344 - accuracy: 0.8940 - val_loss: 0.3807 - val_accuracy: 0.9305\nEpoch 75/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4363 - accuracy: 0.8829 - val_loss: 0.3803 - val_accuracy: 0.9305\nEpoch 76/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4338 - accuracy: 0.8848 - val_loss: 0.3798 - val_accuracy: 0.9324\nEpoch 77/100\n102/102 [==============================] - 35s 342ms/step - loss: 0.4370 - accuracy: 0.8854 - val_loss: 0.3793 - val_accuracy: 0.9305\nEpoch 78/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4295 - accuracy: 0.8872 - val_loss: 0.3788 - val_accuracy: 0.9305\nEpoch 79/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4376 - accuracy: 0.8838 - val_loss: 0.3784 - val_accuracy: 0.9305\nEpoch 80/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4363 - accuracy: 0.8789 - val_loss: 0.3779 - val_accuracy: 0.9324\nEpoch 81/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4344 - accuracy: 0.8749 - val_loss: 0.3775 - val_accuracy: 0.9324\nEpoch 82/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4309 - accuracy: 0.8964 - val_loss: 0.3771 - val_accuracy: 0.9324\nEpoch 83/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4309 - accuracy: 0.8891 - val_loss: 0.3765 - val_accuracy: 0.9324\nEpoch 84/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4290 - accuracy: 0.8884 - val_loss: 0.3761 - val_accuracy: 0.9324\nEpoch 85/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4353 - accuracy: 0.8805 - val_loss: 0.3756 - val_accuracy: 0.9342\nEpoch 86/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4320 - accuracy: 0.8801 - val_loss: 0.3751 - val_accuracy: 0.9305\nEpoch 87/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4290 - accuracy: 0.8869 - val_loss: 0.3746 - val_accuracy: 0.9342\nEpoch 88/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4291 - accuracy: 0.8921 - val_loss: 0.3741 - val_accuracy: 0.9324\nEpoch 89/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4321 - accuracy: 0.8857 - val_loss: 0.3737 - val_accuracy: 0.9324\nEpoch 90/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4285 - accuracy: 0.8848 - val_loss: 0.3732 - val_accuracy: 0.9324\nEpoch 91/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4304 - accuracy: 0.8811 - val_loss: 0.3728 - val_accuracy: 0.9342\nEpoch 92/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4248 - accuracy: 0.8857 - val_loss: 0.3723 - val_accuracy: 0.9342\nEpoch 93/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.4328 - accuracy: 0.8780 - val_loss: 0.3719 - val_accuracy: 0.9324\nEpoch 94/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4268 - accuracy: 0.8857 - val_loss: 0.3714 - val_accuracy: 0.9324\nEpoch 95/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4297 - accuracy: 0.8777 - val_loss: 0.3710 - val_accuracy: 0.9324\nEpoch 96/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4312 - accuracy: 0.8792 - val_loss: 0.3706 - val_accuracy: 0.9342\nEpoch 97/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.4339 - accuracy: 0.8731 - val_loss: 0.3702 - val_accuracy: 0.9342\nEpoch 98/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4294 - accuracy: 0.8805 - val_loss: 0.3698 - val_accuracy: 0.9324\nEpoch 99/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4287 - accuracy: 0.8829 - val_loss: 0.3693 - val_accuracy: 0.9324\nEpoch 100/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4276 - accuracy: 0.8814 - val_loss: 0.3688 - val_accuracy: 0.9342\nMengkompilasi model...\nfold 7\nTraining head model...\nEpoch 1/100\n102/102 [==============================] - 39s 371ms/step - loss: 0.4274 - accuracy: 0.8841 - val_loss: 0.3788 - val_accuracy: 0.9250\nEpoch 2/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4270 - accuracy: 0.8829 - val_loss: 0.3786 - val_accuracy: 0.9250\nEpoch 3/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4229 - accuracy: 0.8921 - val_loss: 0.3781 - val_accuracy: 0.9250\nEpoch 4/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4245 - accuracy: 0.8866 - val_loss: 0.3778 - val_accuracy: 0.9250\nEpoch 5/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.4242 - accuracy: 0.8851 - val_loss: 0.3776 - val_accuracy: 0.9232\nEpoch 6/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4222 - accuracy: 0.8881 - val_loss: 0.3770 - val_accuracy: 0.9250\nEpoch 7/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.4239 - accuracy: 0.8814 - val_loss: 0.3766 - val_accuracy: 0.9250\nEpoch 8/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4232 - accuracy: 0.8860 - val_loss: 0.3766 - val_accuracy: 0.9232\nEpoch 9/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4242 - accuracy: 0.8820 - val_loss: 0.3762 - val_accuracy: 0.9232\nEpoch 10/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4223 - accuracy: 0.8918 - val_loss: 0.3758 - val_accuracy: 0.9232\nEpoch 11/100\n102/102 [==============================] - 37s 358ms/step - loss: 0.4181 - accuracy: 0.8937 - val_loss: 0.3754 - val_accuracy: 0.9232\nEpoch 12/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.4169 - accuracy: 0.8931 - val_loss: 0.3749 - val_accuracy: 0.9232\nEpoch 13/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4202 - accuracy: 0.8801 - val_loss: 0.3743 - val_accuracy: 0.9232\nEpoch 14/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4234 - accuracy: 0.8826 - val_loss: 0.3739 - val_accuracy: 0.9232\nEpoch 15/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4216 - accuracy: 0.8786 - val_loss: 0.3735 - val_accuracy: 0.9232\nEpoch 16/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4204 - accuracy: 0.8814 - val_loss: 0.3730 - val_accuracy: 0.9232\nEpoch 17/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4218 - accuracy: 0.8854 - val_loss: 0.3727 - val_accuracy: 0.9232\nEpoch 18/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4218 - accuracy: 0.8795 - val_loss: 0.3723 - val_accuracy: 0.9232\nEpoch 19/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4215 - accuracy: 0.8829 - val_loss: 0.3718 - val_accuracy: 0.9232\nEpoch 20/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4148 - accuracy: 0.8808 - val_loss: 0.3711 - val_accuracy: 0.9232\nEpoch 21/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4218 - accuracy: 0.8789 - val_loss: 0.3708 - val_accuracy: 0.9232\nEpoch 22/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4209 - accuracy: 0.8765 - val_loss: 0.3705 - val_accuracy: 0.9232\nEpoch 23/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4143 - accuracy: 0.8903 - val_loss: 0.3698 - val_accuracy: 0.9232\nEpoch 24/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4133 - accuracy: 0.8909 - val_loss: 0.3696 - val_accuracy: 0.9232\nEpoch 25/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4132 - accuracy: 0.8924 - val_loss: 0.3693 - val_accuracy: 0.9232\nEpoch 26/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4179 - accuracy: 0.8774 - val_loss: 0.3690 - val_accuracy: 0.9232\nEpoch 27/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4151 - accuracy: 0.8832 - val_loss: 0.3686 - val_accuracy: 0.9232\nEpoch 28/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4136 - accuracy: 0.8875 - val_loss: 0.3681 - val_accuracy: 0.9232\nEpoch 29/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4141 - accuracy: 0.8900 - val_loss: 0.3679 - val_accuracy: 0.9232\nEpoch 30/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4149 - accuracy: 0.8838 - val_loss: 0.3674 - val_accuracy: 0.9232\nEpoch 31/100\n102/102 [==============================] - 37s 357ms/step - loss: 0.4205 - accuracy: 0.8829 - val_loss: 0.3668 - val_accuracy: 0.9232\nEpoch 32/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4144 - accuracy: 0.8817 - val_loss: 0.3665 - val_accuracy: 0.9232\nEpoch 33/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4155 - accuracy: 0.8832 - val_loss: 0.3661 - val_accuracy: 0.9232\nEpoch 34/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4111 - accuracy: 0.8869 - val_loss: 0.3657 - val_accuracy: 0.9232\nEpoch 35/100\n102/102 [==============================] - 35s 341ms/step - loss: 0.4131 - accuracy: 0.8888 - val_loss: 0.3652 - val_accuracy: 0.9232\nEpoch 36/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.4166 - accuracy: 0.8888 - val_loss: 0.3649 - val_accuracy: 0.9232\nEpoch 37/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4166 - accuracy: 0.8771 - val_loss: 0.3644 - val_accuracy: 0.9232\nEpoch 38/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.4114 - accuracy: 0.8851 - val_loss: 0.3638 - val_accuracy: 0.9232\nEpoch 39/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4069 - accuracy: 0.8857 - val_loss: 0.3634 - val_accuracy: 0.9232\nEpoch 40/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4130 - accuracy: 0.8826 - val_loss: 0.3630 - val_accuracy: 0.9232\nEpoch 41/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4133 - accuracy: 0.8823 - val_loss: 0.3625 - val_accuracy: 0.9232\nEpoch 42/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4107 - accuracy: 0.8844 - val_loss: 0.3621 - val_accuracy: 0.9232\nEpoch 43/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.4123 - accuracy: 0.8832 - val_loss: 0.3617 - val_accuracy: 0.9232\nEpoch 44/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4078 - accuracy: 0.8884 - val_loss: 0.3614 - val_accuracy: 0.9232\nEpoch 45/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4053 - accuracy: 0.8823 - val_loss: 0.3609 - val_accuracy: 0.9232\nEpoch 46/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.4039 - accuracy: 0.8974 - val_loss: 0.3605 - val_accuracy: 0.9232\nEpoch 47/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.4031 - accuracy: 0.8903 - val_loss: 0.3602 - val_accuracy: 0.9232\nEpoch 48/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4123 - accuracy: 0.8851 - val_loss: 0.3598 - val_accuracy: 0.9232\nEpoch 49/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.4124 - accuracy: 0.8894 - val_loss: 0.3593 - val_accuracy: 0.9232\nEpoch 50/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4059 - accuracy: 0.8918 - val_loss: 0.3586 - val_accuracy: 0.9232\nEpoch 51/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4039 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.9232\nEpoch 52/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4055 - accuracy: 0.8878 - val_loss: 0.3577 - val_accuracy: 0.9232\nEpoch 53/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4062 - accuracy: 0.8921 - val_loss: 0.3573 - val_accuracy: 0.9232\nEpoch 54/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.4035 - accuracy: 0.8878 - val_loss: 0.3571 - val_accuracy: 0.9232\nEpoch 55/100\n102/102 [==============================] - 37s 359ms/step - loss: 0.4045 - accuracy: 0.8921 - val_loss: 0.3566 - val_accuracy: 0.9232\nEpoch 56/100\n102/102 [==============================] - 37s 361ms/step - loss: 0.4062 - accuracy: 0.8881 - val_loss: 0.3564 - val_accuracy: 0.9232\nEpoch 57/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.4060 - accuracy: 0.8869 - val_loss: 0.3562 - val_accuracy: 0.9232\nEpoch 58/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.4030 - accuracy: 0.8884 - val_loss: 0.3557 - val_accuracy: 0.9232\nEpoch 59/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.4030 - accuracy: 0.8909 - val_loss: 0.3551 - val_accuracy: 0.9232\nEpoch 60/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4039 - accuracy: 0.8903 - val_loss: 0.3545 - val_accuracy: 0.9232\nEpoch 61/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.3973 - accuracy: 0.8931 - val_loss: 0.3546 - val_accuracy: 0.9232\nEpoch 62/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4035 - accuracy: 0.8912 - val_loss: 0.3542 - val_accuracy: 0.9232\nEpoch 63/100\n102/102 [==============================] - 35s 345ms/step - loss: 0.4042 - accuracy: 0.8832 - val_loss: 0.3535 - val_accuracy: 0.9232\nEpoch 64/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.3962 - accuracy: 0.8940 - val_loss: 0.3532 - val_accuracy: 0.9232\nEpoch 65/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.4001 - accuracy: 0.8921 - val_loss: 0.3529 - val_accuracy: 0.9232\nEpoch 66/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4016 - accuracy: 0.8894 - val_loss: 0.3527 - val_accuracy: 0.9232\nEpoch 67/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.3969 - accuracy: 0.8900 - val_loss: 0.3521 - val_accuracy: 0.9232\nEpoch 68/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.3945 - accuracy: 0.8906 - val_loss: 0.3518 - val_accuracy: 0.9232\nEpoch 69/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.3973 - accuracy: 0.8946 - val_loss: 0.3513 - val_accuracy: 0.9232\nEpoch 70/100\n102/102 [==============================] - 35s 344ms/step - loss: 0.4019 - accuracy: 0.8826 - val_loss: 0.3507 - val_accuracy: 0.9232\nEpoch 71/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.3984 - accuracy: 0.8894 - val_loss: 0.3503 - val_accuracy: 0.9232\nEpoch 72/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.3980 - accuracy: 0.8940 - val_loss: 0.3498 - val_accuracy: 0.9232\nEpoch 73/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.3970 - accuracy: 0.8924 - val_loss: 0.3494 - val_accuracy: 0.9232\nEpoch 74/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.3968 - accuracy: 0.8884 - val_loss: 0.3491 - val_accuracy: 0.9232\nEpoch 75/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.3989 - accuracy: 0.8900 - val_loss: 0.3493 - val_accuracy: 0.9232\nEpoch 76/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.3980 - accuracy: 0.8903 - val_loss: 0.3489 - val_accuracy: 0.9232\nEpoch 77/100\n102/102 [==============================] - 35s 346ms/step - loss: 0.3991 - accuracy: 0.8854 - val_loss: 0.3483 - val_accuracy: 0.9232\nEpoch 78/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3920 - accuracy: 0.8998 - val_loss: 0.3475 - val_accuracy: 0.9232\nEpoch 79/100\n102/102 [==============================] - 36s 348ms/step - loss: 0.3998 - accuracy: 0.8835 - val_loss: 0.3472 - val_accuracy: 0.9232\nEpoch 80/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.3977 - accuracy: 0.8940 - val_loss: 0.3469 - val_accuracy: 0.9232\nEpoch 81/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.3952 - accuracy: 0.8940 - val_loss: 0.3466 - val_accuracy: 0.9232\nEpoch 82/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.3928 - accuracy: 0.8970 - val_loss: 0.3463 - val_accuracy: 0.9232\nEpoch 83/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.3910 - accuracy: 0.8955 - val_loss: 0.3457 - val_accuracy: 0.9232\nEpoch 84/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3973 - accuracy: 0.8924 - val_loss: 0.3450 - val_accuracy: 0.9232\nEpoch 85/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.3995 - accuracy: 0.8823 - val_loss: 0.3448 - val_accuracy: 0.9232\nEpoch 86/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3961 - accuracy: 0.8881 - val_loss: 0.3445 - val_accuracy: 0.9232\nEpoch 87/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.3924 - accuracy: 0.8943 - val_loss: 0.3442 - val_accuracy: 0.9232\nEpoch 88/100\n102/102 [==============================] - 36s 356ms/step - loss: 0.3943 - accuracy: 0.8881 - val_loss: 0.3437 - val_accuracy: 0.9232\nEpoch 89/100\n102/102 [==============================] - 35s 347ms/step - loss: 0.3947 - accuracy: 0.8986 - val_loss: 0.3436 - val_accuracy: 0.9232\nEpoch 90/100\n102/102 [==============================] - 36s 354ms/step - loss: 0.3936 - accuracy: 0.8888 - val_loss: 0.3434 - val_accuracy: 0.9232\nEpoch 91/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.3891 - accuracy: 0.8955 - val_loss: 0.3429 - val_accuracy: 0.9232\nEpoch 92/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3938 - accuracy: 0.8949 - val_loss: 0.3426 - val_accuracy: 0.9232\nEpoch 93/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3972 - accuracy: 0.8888 - val_loss: 0.3424 - val_accuracy: 0.9232\nEpoch 94/100\n102/102 [==============================] - 36s 350ms/step - loss: 0.3922 - accuracy: 0.8934 - val_loss: 0.3418 - val_accuracy: 0.9232\nEpoch 95/100\n102/102 [==============================] - 36s 352ms/step - loss: 0.3930 - accuracy: 0.8915 - val_loss: 0.3416 - val_accuracy: 0.9232\nEpoch 96/100\n102/102 [==============================] - 36s 347ms/step - loss: 0.3928 - accuracy: 0.8906 - val_loss: 0.3412 - val_accuracy: 0.9232\nEpoch 97/100\n102/102 [==============================] - 36s 351ms/step - loss: 0.3960 - accuracy: 0.8903 - val_loss: 0.3408 - val_accuracy: 0.9232\nEpoch 98/100\n102/102 [==============================] - 36s 353ms/step - loss: 0.3882 - accuracy: 0.8946 - val_loss: 0.3403 - val_accuracy: 0.9232\nEpoch 99/100\n102/102 [==============================] - 36s 355ms/step - loss: 0.3937 - accuracy: 0.8958 - val_loss: 0.3402 - val_accuracy: 0.9232\nEpoch 100/100\n102/102 [==============================] - 36s 349ms/step - loss: 0.3872 - accuracy: 0.8983 - val_loss: 0.3397 - val_accuracy: 0.9232\nFold 1 - Accuracy: 0.8321\nFold 2 - Accuracy: 0.9069\nFold 3 - Accuracy: 0.9124\nFold 4 - Accuracy: 0.9307\nFold 5 - Accuracy: 0.9269\nFold 6 - Accuracy: 0.9342\nFold 7 - Accuracy: 0.9232\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluasi Jaringan\n","metadata":{"id":"3Mqe8nNB7lp8"}},{"cell_type":"code","source":"average_accuracy = np.mean(fold_accuracies)\nprint(f\"Average Accuracy: {average_accuracy:.4f}\")\n\n# Standard deviation of accuracy across all folds\nstd_accuracy = np.std(fold_accuracies)\nprint(f\"Standard Deviation of Accuracy: {std_accuracy:.4f}\")\n\n# Find the best and worst fold\nbest_fold = np.argmax(fold_accuracies)\nworst_fold = np.argmin(fold_accuracies)\n\nprint(f\"Best Fold: {best_fold + 1} with Accuracy = {fold_accuracies[best_fold]:.4f}\")\nprint(f\"Worst Fold: {worst_fold + 1} with Accuracy = {fold_accuracies[worst_fold]:.4f}\")\n\n# Plot the accuracy per fold\nplt.figure(figsize=(7, 4))\nplt.plot(range(1, len(fold_accuracies) + 1), fold_accuracies, marker='o')\nplt.xlabel('Fold')\nplt.ylabel('Accuracy')\nplt.title('Accuracy per Fold - K-Fold Cross Validation')\nplt.xticks(range(1, len(fold_accuracies) + 1))\nplt.grid(True)\nplt.show()","metadata":{"id":"HLxROO1pg7Em","execution":{"iopub.status.busy":"2023-11-26T12:20:47.067535Z","iopub.execute_input":"2023-11-26T12:20:47.067879Z","iopub.status.idle":"2023-11-26T12:20:47.348007Z","shell.execute_reply.started":"2023-11-26T12:20:47.067845Z","shell.execute_reply":"2023-11-26T12:20:47.347123Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Average Accuracy: 0.9095\nStandard Deviation of Accuracy: 0.0328\nBest Fold: 6 with Accuracy = 0.9342\nWorst Fold: 1 with Accuracy = 0.8321\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 700x400 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}